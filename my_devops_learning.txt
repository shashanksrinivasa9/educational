Some Basic Linux commands:

free --> The free command gives information about used and unused memory usage and swap memory of a system
top ---> The “top” command provides a dynamic real-time view of a running system.
uptime -->  Tell how long the Linux system has been running. w command – Show who is logged on and what they are doing including the uptime of a Linux box.
df --->disk free --> The df command stands for "disk-free," and shows available and used disk space on the Linux system. df -T shows the disk usage along with each block's filesystem type (e.g., xfs, ext2, ext3, btrfs, etc.)
du --> disk usage ---> The du (disk usage) command measures the disk space occupied by files or directories. By default, it measures the current directory and all its subdirectories, printing totals in blocks for each, with a grand total at the bottom.
uname ---> The uname is a command line tool most commonly used to determine the processor architecture, the system hostname and the version of the kernel running on the system.
cat /etc/osrelease
curl curl is a command-line tool to transfer data to or from a server, using any of the supported protocols (HTTP, FTP, IMAP, POP3, SCP, SFTP, SMTP, TFTP, TELNET, LDAP, or FILE). curl is powered by Libcurl. This tool is preferred for automation since it is designed to work without user interaction. curl can transfer multiple files at once. 
Syntax:  

curl [options] [URL...]
URL: The most basic use of curl is typing the command followed by the URL.  

curl https://www.geeksforgeeks.org
This should display the content of the URL on the terminal. The URL syntax is protocol dependent and multiple URLs can be written as sets like: 

wget ---> file downloader ,non-interactive(meaning can work when the user is offline )
mv source_file destination file <command to rename a file.>
adduser <username> creates a new user in the linux file system.
chown <options> like -R user:group <file name ,absoulte path is needed if we are in a different location> ,this is used to change the owner and group of a file
rm -rf is the command to remove a file from linux file system.
ln -s <file1> <file2> --> this is the command to create soft link of file1 as file 2. whenever file2 is executed internally file 1 also executed.
su <username> is used to switch the user in the terminal
netstat -plnt is the command to check if a service is up and running or not.
ps -ef <command to check what processes are running>


question:

connect from your linux instance to docker instance .(docker instance doesnt have key)

solution :

open your linux machine 

open your docker machine

in your linux machine, create keys (public and prviate) using key gen command

from the linux machine, copy the public key in the registered key folder of docker machine.  we need to append to the already exisitng key.

then connect to the docker machine from the linux machine using that key

Cron jobs are used to run jobs at a specified scheduled time

the syntax is as follows : * * * * * (5 stars) representing minute,hour,day,month,day of the week

minute (0-59),hour(0-23),day of the month(1-31),month (1-12),day of the week(0-7, both 0 and 7 representing sunday)

crontab guru --is the online tool to check cronjob syntax

crontab -e/l  -- is the command to check and edit our own cronjob

Shell Scripting Basics :

read command  is used to give parameters through command prompt

$* --> takes input in a string separated by a space by default
$# ---> prints the number of inputs given via command line
$? ---> checks the status of the previous command 0 for pass 1 for fail
$0 -->this represents the name of the script.
$1 and $2 are magic variables that will be assigned to the first and second argument passed to the script
-z <returns true if the length of the command prompt variable is o>

&& and operator
|| or operator

if[conditon]; then
 statement
 else
 statement
fi

while checking conditions use == for comparing string values.

syntax to print date with time is as below

here it is printing date, month, year,minutes and hours in the order from left to right
date + %m_%d_%y_%H_%M_%S 

version controller:

2 types of version controllinjg mechanisms are present

centralized version control and distriubuted version control
in centralized version control, developers work on a central server directly where as in a distributed version control system

each developer will have a local copy of the central server. In centralized version control systems we will need internet and there is a

chance of single point of failure. whereas in a distributed version control system once we have the repo copy locally we can make changes within
our local machine and parallel development is possible even with out internet (only in cases of push,pull and clone we will need internet)

git concepts:

create folder

make changes in any file(example text file)
git init
git status
git add.

git commit -m "message"

push your changes to repo (remote repo)

git push origin master

create git hib account
create repo with same folder name

git hub token :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 to push into a new repo

git remote add <url> to manage the repo from terminal


git clone < url> ssh or https to copy the projecty to local machine

git fork command to copy entire repo from another user to our repo

GIT branching

feature branch to staging then staging to pre production and then pre production to master branch

git branch --> command to see which branch we are standing (with * and green)
git branch -a shows all the branches in our account.

git branch <branch name> command to create new branch

git checkout <branchname> to jump to another branch

to bring changes from one branch to other go to that branch and raise a pull request. Once a pull request is raised , we can then confirm merge request.

once merge request is confirmed we will get the changes from the newly create merge to the master branch.

git checkout -b <branch_name> will create a new branch and will move the prompt to that branch together.

git branch -d  <branch name > deletes the branch from local machine

git push origin -d <branch name > will delete the branch from the remote account also.

git fetch --> will bring in the new branch from remote repo to local with out actually bringing in the changes

difference between git fetch and git pull is git pull will bring the branch and file changes,,while fetch will only map the new branch but not the changes

git tag -l <give the list of all tags>

git tag <tag_name> creates a new tag

git tag -a <tag name> -m "message" creates anointed tag which is a special tag
git push origin <tag_name> will push the tag to the git hub

the above will create tag on the latest commit. to create a tag on the previous releases,

we will do later tagging

to do it we use below command
git tag <tag_nam> <commit_id first 6 digits>

git tag -d <tag name > will remove tag from local machine
git push origin :ref/tag/tag_name will remove tag from the remote repo also.

git checkout <filename> will undo the changes

to undo previous changes we need to use following commands

git checkout
git revert
git reset

the areas of GIT are working |(local repo) ---> staging/indexing (after adding )  --->commit (after commiting) ---> git push (after pushing the file to github)

to move files from working area to staging area we use git add . further to move files from staging to commit area we use git commit 

to move files from commit area to git hub we push the code using git push <origin> <master >

git checkout <file name> will revert the changes at the working stage.

git checkout <file name1> <filename2> <filename3> will undo changes of multiple files.

git diff --> will give changes between commit area file and local file  <the changes added will be displayed in green and then changes removed will be shown in red>

to undo changes from commit area back to working directory we use git reset command

git reset is of 3 types 

git reset --soft Head~1 ---> this remove the commitn message and bring the file back to staging area from commit .so all we need is to commit the change again.file changes
will still be present inside it.

git reset -- mixed Head~1 --mixed can be skipped and git reset by defualt is --mixed .here the file changes are brought from commit area to working directory
changes will be present in the working directory but we will have to add them and then commit if we want any action on them

git reset --hard Head~1  --> will remove the file from working directory also.

git revert <commitsha> first 6 olr 7 letters of commit id will revert the changes from the git hub all the way upto working directory meaning it will

completely remove all the changes. jt will also commit a small change indicating the revert operation.

git commit --amend -m "message name " will change the commit message from a previous commit ---> will only work for the latest message

git rebase -i HEAD~<message number > ..will amend commit messages of  the commit number from the head.

git log--oneline -->will give commit message and then id in a single line

git rebase -i HEAD<5> and then select "S" option for sqaushing..squashing will meld different commits together.


while we raise the pull request we get 3 options git merge ,git squash and merge, git rebase and merge in ui

to see in details

suppose in master branch there are 3 commits , then a new implementation came in a new branch called "feature " and it has 2 commits.


then later on in the master branch if a new hot fix , we need to understand what is the result of it if we use the above 3 commands.

to merge changes between 2 branches within UI, we go to the branch from where we need to merge the changes  and do a pull request. then give base branch 

and destination branch.base and compare in case of github

the problem with git squash and merge is we lose the development history and commit data as it willo merge all the commits into a single message . 

with git merge , even then we get the commit history and development history,the  data will not be linear and feature branch data and master branch data

get jumbled up. so the problem is not completely solved even with this approach

with git rebase command, a clear development history and commit history is obtained. so, it is always the best to use this option if development history
has to be kept intact

git cherry-pick <commitsha> will bring a specific commit from one branch to other.

git stash command stores our working copy temporarily for us to switch branches and then start from the reference poiint again

git stash apply --is to save the working copy
git stash pop -- is to remnove then stashing we need to use stash id in this command.

git stash will only work on the existing files but not new files.

Branching strategy in GIT:

release  --> staging  -->preproduction -->production  

.gitignore file in git environment will add files that have to be avoided for committing


Jenkins:

To install jenkins we need Java jdk 11 first then we have to install jenkins
we can then start jenkins by jenkis service start

jenkins is supported only by jenkins 11

to see java version type java -version.please make sure only java 11 is supported.

In windows, afterinstalling Java 11 edit the environment variables

add envirnoment variable name as: JAVA_HOME and give value as its path .change this in user setting of environment variable.

we will also have to edit the PATH variable by typing %JAVA_HOME%\bin PATH  is a separate environment variable.

to run jenkins in windows we need to type java - jar jenkins.war from the command prompt. It has to be openend from where the jenkins war file

is installed. by default jetty is the server that runs inside jenkins

To start jenkins in browser we use port localhost:8080 .password for jenkins is stored inside a sceret folder in the jenkins the installation

folder path.  

to install jenkins in AMI linux machine :
sudo yum install jenkins -y

to check whether jenkins is running or not in an AMI linux machine we use the command

some basic jobs in jenkins that we see are:

print hello message

pull code from github and work with it --integraten github with jenkins

create sample java project and work on it.

to create a job in jenkins, we first start its service using command jenkins service start.

we open the jenkins server on the port 8080. For this to open we will have to edit our security group policies to enable tcp traffic on 8080 port.

we    go to the browser and type our ec2 instance id and jenkins port number 8080 separated with colon,

the first job we create is a basic hello world job. In this job, we click on create new button and then update our job name .

We also give the command to be run for our job at the bottom section. here we can give echo "hello world" oneimpiortant option to select

is runtime env which we need to select as bash shell. so this can be considered as a very basic job.

second job is to integrate a git repository with jenkins and run it on jenkins. to do this, we should create a new job and then configure our setting\

by adding our git repo connection link(used for cloning the git repos give the https link instead of the ssh one).  we also need to give our git hub credential to pull the git changes.

then we have to make sure we install git in our ec2 instance (if not present we need\

to install it by using command sudo yum git install -y. Along with congifuiring our git repo within the job page, we also need to change it in

the main jenkins page by editing manage jenkins page . here it expects the executable path of GIT and if GIT is already installed in our ec2 instance

it automatically pops up there. so it is essential that git is confiugured in these 2 locations mandatorily. p

In Jenkins there is option for us to manage the build trigger automatically. these are of 3 types

they are :

poll scm  ---> here job will be triggered at a specific time if a change is pushed

build periodically ---> job will be triggered in a period after a specific time even without changes by picking latest change

and 

webhook --> job will be triggered immediately if code changes are made .here there is no waiting period as in the case of poll scm.

all the above 3 jobs are cron jobs. poll scm and build periodically are pull based mechanism and webhook is a push based mechanism.

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 --->github token

to create poll scm in configure page of a jenkins job, in the build triggers section select poll scm and provie the necessary cron job schedule

if the job is for periodical do the same but select build periodically

however to select a webhook there is no need to create a cron job .instead select webhook option in the jenkins job page build trigger section
in the git page of the project select webhook option and copy and paste the jenkins url link followed by /github-webhook.this we will get inside
the specific repo setting but not the overall repo.

Maven:

Every project in Maven has a pom.xml file. (project object module) generally, a maven project may have src,test and pom.xml file.

pom.xml file has syntax . 

it primarily has tags like

dependencies, groupid, artifactid( jar file name), scope (when the jar file is used build or test). we can add multiple dependencies.

example is below:

<!-- https://mvnrepository.com/artifact/org.apache.maven/maven-core -->
<dependency>
    <groupId>org.apache.maven</groupId>
    <artifactId>maven-core</artifactId>
    <version>4.0.0-alpha-5</version>
</dependency>

please note that maven architecture contains different repositories , local,remote and central.

.m2 folder in maven architecture is our local repo. any dependency files that are needed for our file will be downloaded from remote repo and copied

into our local repo. If the dependency is not even available in remote repo, it will be kept in cental repo (which is internet).

the remote repositories example are jfrog,nexus,maven in our own linux server and it can be used by entire company staff to download the repo

but if the artifacts are public they are then downloaded from central repo (internet ).  only custom dependencies or jar files can be kept in our 

remote repo.

in pom.xml file, the code we write will help our project download the data from the internet. from next time onwards it will directly use the data
from remote or local repo

Maven project has following lifecycle:
clean
validate --->validates project structure 
compile -->  syntax check 
test ---> executes test classes. 
package ---> final output jars --> they are deployed on to web servers like (apache tom cat )
verify
install
deploy

how will we know our package gives .jar or .war

so in pom.xml file a tag called packaging is present, which givesn info about file is .war or .jar

but by default it is .jar 

we use the lifecycle untill package. so we use mvn <lifecycle COmmand> for the maven project to work for ex maven package

in maven package , validate, compile test and package together .. so if we give maven deploy all the steps will get executed

clean is a default lifecycle. every package step will create a target folder which will have test report folder (final output .war file). when we do target again,

it will remove the previous target folder and create new target folder (default program --> meaning clean). 

use srping3 maven github example (https://github.com/mkyong/spring3-mvc-maven-xml-hello-world) please note all the tags in this link.please understand
that all the dependncies,plugins etc will be added by the developer.

we alson have settings.xml in our maven project, in this we will have info of remote repository like server details ex: id,pwd private key,file permission usage of proxies etc.

to create maven project we use the command : mvn archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4 -DinteractiveMode=false

in the above command archetype:generate group id will create maven project and it will name the project according to the artifactid name which is my-app.

Maven -Jenkins , Apache tomcat-Jenkins integration.

Maven -jenkins:

After creating a maven project using above command, we can push the pom.xml to github. we should then create a free style job in jenkins prefarbly with the same name

as in maven project. Later we should integrate our maven project with the jenkins tool. For this purpose, we have to go to the configure page within the job of our
jenkins page and add our maven github url .we can use the credentials of our github .  Along with this, the most important aspect is also to present the maven
build pipeline stage , example : pasckage: in the build steps which help the jenkins understand the step in maven life cycle until which the 
process will be executed. Please note that a .jar file or .war file will be only be created if we give the package command in the build steps section.

Any command representing the lifecycle before package will only execute the operation until that point only.

A successful package step is the only option for us to see a .jar or .war file inside the target folder and in test report folder. jenkins job results are stored in 
/var/lib/jenkins folder.

Now, a jar file can be executed using the command java -jar <jarfilename>.jar.

But how to create a .war file and deploy it in the server. For this purpose, we will need to understand .war file creation from pom.xml file and know about
apache tom cat server also. 

It is to be noted that the creation of. war or.jar is dependent on packaging tag inside the pom.xml file. So, we can mention .war if we need .war files.(web archive files).

For this purpose, we can fork a java spring boot code from an example github like : https://github.com/mkyong/spring3-mvc-maven-xml-hello-world.

After this we should create a free style job named javaspring anc can integrate our spring boot github with it by giving its url and credentials.

we should again give the option as "package" in the build setting section. So far, we haven't reached the point of deploying our application on to server.

So, for this we use apache tom catn as decribed above. Apache tom cat can be installed in 2 ways .First can be downloaded directly from the apache software

website and second is through command like sudo yum install apache tomcat -y. Whatever possible manner is used the resultant directory contains three important
folders that have to be understood definitely by all. They are 

bin --> it contains tomcat start and stop scripts . tomcat just like jenkins is a special service and has to be started using command 

sudo service tomcat start , stop , sttatus.


conf ---> folder which contains important .xml files like tomcat-users.xml and server.xml which are responsible for username and password details

and changing the port number of the tomcat service. For username and password details ,they are set by default as admin and adminadmin.however, when we start
the tomcat service and give these credentials, it doesnt work. Reason being in the tomcat-users.xml file they are commented. We have to remove the comments 


we should then open the file server.xml and change the server port to 8081 or 82 any port of our choice. This is to make sure the same port is not in 

conflict with the already existing service (mostly like jenkins). After we do these steps we have to restart the tomcat server before using it.

Please note that these files are present in the locations mentioned above when we install the tomcat from the website by downloading the zip file.

instead if we install them directly from the command prompt using the sudo yum install command, then they are (atleast server.xml and tomcat-users.xml)

inside the folder /etc/tomcat .After these settings are done, we load the tomcat page using the public ip address and the port number. Just in the case

of any other service even with this we have to edit inbound rules in the AWS security group page by adding 8081 port.

Please note that as in the case of maven and git, we will need a special plugin to integrate tomcat with our jenkins too. It is called as

"Deploy to container " it has to be specially downloaded from available plugins.the input to the plugin is the location of the .war file in the 

Maven project. usually present in target folder. so we need to give itn as **/target/*.war

General  diagram of the flow so far along with the details.

Git(for changin files) -->github (online repo to see our files ) ---> jenkins (for integration purpose){maven(for compilation and artifact creation like 
.war/.jar),apache tomcat (for .war file deploying).

Contiuning with above process, after creating the apache tomcat on an isntance along with Jenkins, try and setup tomcat server on a separate instance.

we can take 2 git hub projects for practice 1) petclinic 2) game of life. the idea is simple. to fork the code in those repos to your github.

integrate github,maven with the jenkins instance of our ec2 instance and build a .war file after which it has to be deployed using tomcat.

some examples where the application may not get deployed in older ways where the process follows like  this  

github --> jenkins  (git hub, maven, tomcat integration), are 1) both tomcat and jenkins seerver running on the same instance so we need to run them separately

2) increasing the heap memory of jenkins instance in the file /var/lib/jenkins (sys/config and jenkins file ) .in this file we need to update the command as below.

JENKINS_JAVA_OPTIONS="-Djava.awt.headless=true -Xmx1024m -XX:MaxPermSize=512m

if the above 2 wont work then we can also go for changing the type of ami instance from t2.micro 2gb t2.micro 8gb (please note that this is chargeable).

-----
Sonatype nexus and Jfrog:

There are repositories in market that help us to keep a copy of of our .war/.jar files which can be reused as and when required ( we can also docker files,helm chartsetc).

both Sonatype and Nexus have 2 flavours ---> oss and pro  OSS is free and Pro is chargeable.

Nexus can also be implemented in 2 types free style pipeline Also for nexus we will need medium instance  (t2.medium). please create instance on ubuntu.

So, the basic job of nexus is to take back up of artifacts and store it in its repository.

To install nexus follow the material . After nexus installation is done and user and group are added according to the document,

we need to add nexus service as part of the linux bootup. we need to add a softlink for nexus folder that we created to the init.d of etc folder.

in the nexus folder, we change the file called nexus.rc which is inside the bin folder and we add user there. Please note that the default
port in which our nexus service runs is 8081.So,we have to edit our instance's security group to enable this service.

once the port is up and running we can open nexus repo in our server and create a sampple repo for our purpose.After that, to copy the artifacts
we integrate the same through jenkins.

For this purpose, we need to install plugin for nexus to integrate with jenkins. which in "nexus platform". After this is done we need to check for
nexus repository manager publisher option in build steps . please note that maven goal and nexus publisher are present in build steps where as deploy to container

for tomcat is present in post build steps. so we select nexus publisher manager option here. Also, we have to add credentials for nexus in the manage

jenkins section of the main dashboard also. We should give nexus url, add username and password and apply.

After adding the credentials in the main jenkins dashboard, we should come back to configuring our job where we added the fields for build options
please tc  that the .war file that we will give should have full path.Also, the build version number can be kept generic as  ${BUILD_NUMBER}.

Alos, this is helpful to automatically trigger build once there is an update on the version number . 

Please note a basic difference between free style and pipeline jobs is configuring the fields of the tags from pom.xml file has to be done manually everytime

there is a change in the pom.xml in free style jobs.where as pipeline jobs can handle multiple complexities on its own.

After triggering the build and once that is successful , our projects artifacts get uploaded on to the nexus repository.



Sonarqube:

<Space left for sonarqube>







-------------------

Downstream and Updstream Jobs : By adding the option Build after other projects are built in Build triggers section of a job and specifying the Job name

we can add the upstream job Similarly  In post build actions , we can add the job name in Build other projects to trigger a downstream job.

Junit test report:

We can see the results of the job in a GUI way on the main page of the Jenkins page by setting up Run JUNIT tests in post build actions.

Here we need to give the input as .xml file present in the surefire folder of the workspace directory.
------

Dashboard UI: In case of seeing details related to multiple jobs at one place, we make use of a plugin called Dashboard. This will help us see the 

details of various jobs in a much better graphic presentable format. This is present after we click + symbol just beside main page .We should then
add the details accordingly.

------------

Msster-Slave architecture:
There is always dangerof loading the main Jenkins server by running all jobs in it. Instead of that we can make use of slave servers Slave servers
run jobs inside them and provide results . Slaves will remain active as long as the master is alive.We also have to launch slave as soon as it is down

To configure Master-Slave architecture, we will have to click managejenkins-->Nodes-->+ New Node -->Node name and select permanent agent .

After this please unserstand a very imoportant field knownn as Number of executors. This indicates the total number of jobs that can be run on a 

server. This generally depends on the number of CPU cores. We also have to give the root remote directory (create an ec2 isntance andn create a new folder

inside it) this is the folder where our work will be stored.Labels/Usage options are staright fwd(can keep use this node as much as possible)
In the launch method,  try and select launch by ssh agent.

We then have to give credentials ,Host (Host is EC2 instance public IP address) and  user name is ec2-user. After that we also have to copy and paste 

our .pem key from the place where we download it during instance creation. we can then save and apply. The master node is called as Built-In Node and the slave 

node will be below it. Now the most important thing is to configure the slave node in the Built in node job. For this purpose, in the General section  of the

jenkins main job page, we select "Restrict where this project can be run" option and give our slave job ID/Name. With this setting enabled, any triggers on the

main job will be redirected to the slave jobs.

Pipeline Jobs vs Freestyle Jobs: 

All this while we have been discussing about the free style jobs. Freestyle jobs carry an issue of not being able to dynamically

update the content of pom.xml or parse json or yaml files on its own.For example, everytime there is an update related to a project

and if maven triggeres a new .war file, it is not possible with freestyle jobs to update them In such scnearios we use the pipeline jobs.

We use something called DSL (Domain specific langauge) to write a code to perform pipeline jobs

One example of DSL is groovy.This is called as pipeline as code.But please remember that this is used only when even pipeline syntax cant be 

implemented.But generally, we use pipeline code with pipeline syntax.

Pipeline jobs run with a syntax.

Pipeline code also has 2 types of syntax:

1) Scriptbased pipleline -->old ,slow,takes more executors
2) Declarative -->new, fast, less executors, clear readble

Script based code looks like below

node{
stage(){
}
stage(){
}
stage(){
}
-----

Declarative syntax looks like :
pipeline{
agent any;
stages{
	stage('clone'){
		steps{
			----
			}
		}
	stage('build'){
		steps{
			----
			}
		}
	stage('deploy'
	 ){
		steps{
			----
			}
		}
	}
}

There are again 2 ways to write this code 1) pipeline script 2) Groovy sandbox

This is seen after we create a new job and this time selecting pipeline instead of freestyle

It can be seen that with this method we will get only 3 options as opposed to 6 in freestyle

These options are General,Advanced project and Pipeline.

Then we click pipeline--> we select pipeline script in definiton and then uncheck grrovy sandbox at the bottom.

Pipeline script from SCM is also similar but instead of writing directly on the UI we write the code in a jenkins file and upload it to 

our GIT We keep that GIT link the project configuration.


How to write multiple jobs at a time.. We get pipeline syntax(basic) at the configuration page.Once we click that we get a snippet generator

We can use the tab beside the snippet generator sample step and from that select build:Build a Job option.We also have to give the project/jobs
to be built and then at the bottom click on Generate pipeline script.This will automatically generate a syntax that can be copied and pasted in the 
script section of the pipeline option in the pipeline job that we created.After this if we trigger build on the pipeline job, it will internally
trigger all the jobs that are part of the script section in the pipeline option of our job.

In this way we can trigger builds  on steps related to GIT cloning, Maven war file generation, Deployig this .war file in to tomcat amnd also .

For this example in created, Pipeline pipeline_project_complete in jenkins and wrote the pipeline code for it.

copying this .war file into a nexus repository.In pipeline jobs, we dont see "workspace" folder in the UI as we get it in the Free style jobs

So, for this purpose we have to make use of the step "Archive srtifacts" within the pipeline code in pipeline syntax section.

From this we select the archive artifacts option and then we also select the files to archive and put the content in it as /target/*.war and then we click

generate syntax just in case of other steps in the pipeline syntax generattion. we get the syntax as "archiveArtifacts artifacts: 'target/*.war', followSymlinks: false"

We take this syntax and we add a new step in our original pipeline code and we name it artifact (stage) and in the steps section we copy this code.Please 

remember to follow the flow correctly i.e first step is to clone the build, second is to make .war file that is use maven and then artifact the .war file.

After artifact step is done , we have to deploy the .war file in to tomcat server. For this purpose we use curl command.

curl command contains options like -v and -t in our command which represent verbose and upload.

curl -v -u credentials like admin:admin -T <source> <dest> here <source> represents the .war file location in our jenkins server and dest is the ec2 instance where our tom cat is deployed.

so the actual command in our case looks like

sh "curl -v -u admin:admin -T /var/lib/jenkins/workspace/pipeline_project_complete/target/*.war 'http://http://13.52.99.80:8080//manager/html/deploy?path=/pipeline_maven_application&update=true'"

here curl means copy url 

-v verbose --for checking logs in details
-u -- command to take username and pwd

admin:admin credentials for tomcat
-T option informing we are uploading

/var/lib/jenkins --represents the source folder

https://ipaddress and path="" & update=true is the actual destination folder with context path which is the path in which we will upload our. war file

&update =True is option to continue to deploy the changes on to tomcat even when there is no real change in the build.If we do not put this option,

whenever we rerun the above command, it fails as there is nothing to be deployed and its already been taken care.

With the above approach there is an apparent issue  as we are exposing our credentials in this method. So , we have to make sure our credentials are not exposed.

For this purpose we add our Tomcat credentials in the manage jenkins page .Then inside our pipeline job, we create a new pipeline syntax by selecting the option

withcredentials. In this option we select cojoined with username and pwd and give our tomcat ID that we give in the manage jenkins page.

When we generate the pipeline syntax, we get the required output.In our case, its as follows.

withCredentials([usernameColonPassword(credentialsId: 'remotetomcat', variable: 'tomcat_credentials')]) {
    // some block
}

We carefully then copy this content below the deploy step and on top of existing code. We also, remove the admin:admin part of the code from the 

current code so that credentials are not exposed and just give a variable syntax like ${<variable name} Usually the ID which we give in the manage jenkins page.


To read pom.xml file in our pipeline syntax ,the plugin that we need to use is pipeline utility. Within this plugin we use the API named readMavenPom:"pom.xml"

Also, we use findfiles method get the contect from the above pom.xml.The code can fetch the path of artifact, packaging name, id etC.

After that, if the artifacts are present we upload them to nexus . For this we check the code from page upload artifacts to nexus repository.

The plugin  for this "nexusArtifactuploader".The code looks like below.

nexusArtifactUploader(
        nexusVersion: 'nexus3',
        protocol: 'http',
        nexusUrl: 'my.nexus.address',
        groupId: 'com.example',
        version: version,
        repository: 'RepositoryName',
        credentialsId: 'CredentialsId',
        artifacts: [
            [artifactId: projectName,
             classifier: '',
             file: 'my-service-' + version + '.jar',
             type: 'jar']
        ]
     )



Parameterized builds:

In jenkins, we have an option where we can pass inputs to the jenkins job before we make a build. These inputs can be string,branch and tags 

Branch and tags are direct concepts from GIT and represent the same whereas strings can be name of the server on which we might want to deploy

To check this with example we create a free style project and we select "This project is parameterized" option  and then select String Parameter first

After which we give parametername,value and its description. In case of string based parameters we can give inputs as Dev/Test/Prod. So we can begin with Dev.

Also, please note that while triggering build , we will see the option build with parameters instead of just build now as in the case of normal builds.

In this way, we can also add test and prod servers .this is particularly useful whenever we have a condition about what type of server has to run 

what type of build.

In more real world the deploying of applications may happen on tag and branches of GIT hub.So, we create a new project and give it the name branch

based deployment. But, for this , in the option "This project is parameterized" we dont have an option to add parameter related to GIT.For this 

we will have to install plugin related to it. The plugin name is " git parameter" plugin.After installing it we will get the option in parameterized build
section.

We then have to continue and update some important information like Name,Description,Parameter type ---TAG if tag case,Default Value ---> tag value from the 

git repo which we want to deploy in our case it was v1.0 from URL :https://github.com/shashanksrinivasa9/first.  

Then in the SCM section we need to update GIT URL , its credentials and also the branch from where we are trying to pick the tag.

In general for tags this can be updated as refs/tags/${tag}. With this only that tag's code will be downloaded.Once we set this up and click

on build with parameters we will see all tags available in our git hub and we can select our preferred tag. We can cross check that with 

files in our workspace of jenkins server and the files in the git hub repo. 

We can also use Branch in place of Tag and deploy it on to our jenkins.In this case, we use git parameter as "branch" provide the default value

which in our case is Master and then the branch value to be picked up as ${branch} We can then select the branch from which we need the code and it 

will be downloaded.


Git Hub actions :

Git Hub actions is CI/CD tool from GIT . GIT Hub gives the user a hosted zone  called as "Runner". But for every job git hub actions

give the user a new runner or server.So, if we want to create a simple job push some data, script we can use runners.

To start a job in github actions click on actions in git hub and then click set up workflow for yourself.

It opens up a .yaml file. where we can type the code for running the job.

the main syntax looks like below. we can check the syntax in github sybntax workflow page for details.

name:"job name"  --> project name

on:  ---> this will trigger job on only when
	push: --->somebody does a push operation 
	branches: <branch_name> -->on this branch
	pull: --->does a pull request 
	branches: <branch_name> ---> on this branch.
jobs: --->this depects the part of the workflow which is doing a task. for example making the build.
	builds:--> this is where build happens
		runs-on:--> <server name> the runner where the build is run which in our case ubuntu-latest.
	steps:--> this represents the sequence of steps in the process
		uses: actions/checkout@v1  --> line to tell the process to check out the code from the given branch.
		- name: Run a one-line script ---> this is the line to represent the one line scripts like printing or giving any other linux command .this always have 2 fileds name,run.name represents the name of the step and run executes the step.
		run: echo Hello, world!
		-name:java version
		run: echo "java --version"

Sample script that i picked up from jmstech git hub  https://github.com/jmstechhome/jmsth21_first.git  is as follows.

name: CI

on: [push]

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v1
    - name: Run a one-line script
      run: echo Hello, world!
    - name: java version
	
	
	name: web deployment

on: [push]

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v1
    - name: Set up JDK 1.8
      uses: actions/setup-java@v1
      with:
        java-version: 1.8
    - name: Build with Maven
      run: mvn -B package --file pom.xml
    - name: deploy tomcat
      run: curl -v -u ${{ secrets.TOMCAT_USER_NAME }}:${{ secrets.TOMCAT_PASS }} -T /home/runner/work/spring3-mvc-maven-xml-hello-world/spring3-mvc-maven-xml-hello-world/target/spring3-mvc-maven-xml-hello-world-3.0-SNAPSHOT.war 'http://ec2-13-233-173-136.ap-south-1.compute.amazonaws.com:8080/manager/text/deploy?path=/myweb_dev_action&update=true'

	
We have to write this script and commit it on master branch. After that when we see the actions tab again we will notice that the job would

have run completing all the steps .

The default git hub hosted runner will have some default softwares for java,maven,docker.

If it is not present we can install them.

But,instead of default runners we can also use our own runners as with default runners we cant do ssh to them and we cant store session and store data

with in built servers. So, to do these things we use inbuilt servers which are called as self hosted runners.

code-->compile-->validate-->test-->package-->deploy . this is the general CI/CD flow that we should check in github with self hosr runner.

To connect a self hosted runner to github actionswecreate an ec2 instance  go to the repo-->setting-->actions-->runners and new self hosted runner

and then select the OS and architecture as x 64 we want the job to run on in our case linux. then there are commands to be followed and copied in to our instance.

After that the commands to run github actions on selfhosted runners is same as above which runs on ubuntu-latest.

The difference lies with run on option where inn the case of our own runners we use self-hoested instead of ubuntu-latest.

Along, with deploying our .war file in maven, we can also use nexus to copy the .war file in to nexus repository.




	

 




