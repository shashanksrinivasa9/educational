Some Basic Linux commands:

free --> The free command gives information about used and unused memory usage and swap memory of a system
top ---> The “top” command provides a dynamic real-time view of a running system.
uptime -->  Tell how long the Linux system has been running. w command – Show who is logged on and what they are doing including the uptime of a Linux box.
df --->disk free --> The df command stands for "disk-free," and shows available and used disk space on the Linux system. df -T shows the disk usage along with each block's filesystem type (e.g., xfs, ext2, ext3, btrfs, etc.)
du --> disk usage ---> The du (disk usage) command measures the disk space occupied by files or directories. By default, it measures the current directory and all its subdirectories, printing totals in blocks for each, with a grand total at the bottom. 
uname ---> The uname is a command line tool most commonly used to determine the processor architecture, the system hostname and the version of the kernel running on the system.
cat /etc/osrelease
curl curl is a command-line tool to transfer data to or from a server, using any of the supported protocols (HTTP, FTP, IMAP, POP3, SCP, SFTP, SMTP, TFTP, TELNET, LDAP, or FILE). curl is powered by Libcurl. This tool is preferred for automation since it is designed to work without user interaction. curl can transfer multiple files at once. 
Syntax:  
curl [options] [URL...]
URL: The most basic use of curl is typing the command followed by the URL.  
curl https://www.geeksforgeeks.org
This should display the content of the URL on the terminal. The URL syntax is protocol dependent and multiple URLs can be written as sets like: 
wget ---> file downloader ,non-interactive(meaning can work when the user is offline )
mv source_file destination file <command to rename a file.>
adduser <username> creates a new user in the linux file system.
chown <options> like -R user:group <file name ,absoulte path is needed if we are in a different location> ,this is used to change the owner and group of a file
rm -rf is the command to remove a file from linux file system.
ln -s <file1> <file2> --> this is the command to create soft link of file1 as file 2. whenever file2 is executed internally file 1 also executed.
su <username> is used to switch the user in the terminal
netstat -plnt is the command to check if a service is up and running or not.
ps -ef <command to check what processes are running>
useradd -d /home/ansi -m ansi ---> to create a user.
passd <password_name> to create a password for the user 


question:

connect from your linux instance to docker instance .(docker instance doesnt have key)

solution :

open your linux machine 

open your docker machine

in your linux machine, create keys (public and prviate) using key gen command

from the linux machine, copy the public key in the registered key folder of docker machine.  we need to append to the already exisitng key.

then connect to the docker machine from the linux machine using that key

Cron jobs are used to run jobs at a specified scheduled time

the syntax is as follows : * * * * * (5 stars) representing minute,hour,day,month,day of the week

minute (0-59),hour(0-23),day of the month(1-31),month (1-12),day of the week(0-7, both 0 and 7 representing sunday)

crontab guru --is the online tool to check cronjob syntax

crontab -e/l  -- is the command to check and edit our own cronjob

Shell Scripting Basics :

'read' command  is used to give parameters through command prompt

$* --> takes input in a string separated by a space by default
$# ---> prints the number of inputs given via command line
$? ---> checks the status of the previous command 0 for pass 1 for fail
$0 -->this represents the name of the script.
$1 and $2 are magic variables that will be assigned to the first and second argument passed to the script
-z <returns true if the length of the command prompt variable is o>

&& and operator
|| or operator

if[conditon]; then
 statement
 else
 statement
fi

while checking conditions use == for comparing string values.

syntax to print date with time is as below

here it is printing date, month, year,minutes and hours in the order from left to right
date + %m_%d_%y_%H_%M_%S 


What is Localhost?
In computer networking, host means a “server”. Just like you can put a website on the internet by hosting it on a server, you can make your own computer that server. This connection is called loopback. The IP address for that loopback is 127.0.0.1.

If you’ve put a website on the internet before, then you’ve dealt with hosting companies like Heroku, Hostinger, Netlify, and many others. These are what I refer to as “remote hosts” or virtual servers.

If you’ve served a website on your computer so you can test it without connecting to the internet, what you’re dealing with is a localhost.

So, by definition, localhost is the computer or hostname currently making a request to itself. In this case, the computer is also the virtual server.

What is the IP Address 127.0.0.1?
If you want to visit a website, you type the website address to your browser’s address bar, for example, https://freecodecamp.org.

The Domain Name Server (DNS) matches the address to a numeric IP address corresponding to that name. In the case of freeCodeCamp, this IP address is 104.26.2.33. This is how it is done for every website you visit.

Localhost is not an exception to this. So, if you type localhost to your browser’s address bar, it transforms to the IP address 127.0.0.1.

This 127.0.0.1 IP address is reserved for local servers on computers, so you will never find another IP address that starts with 127.

But localhost: what? Or 127.0.0.1: what?
Just like HTTP and HTTPS, the localhost is a protocol. Remember that the website domain name is what follows the http or https, for example, https://www.google.com/ and https://www.freecodecamp.org/.

So, something has to follow localhost: and 127.0.0.1:. That thing is the port number.

version controller:

2 types of version controllinjg mechanisms are present

centralized version control and distriubuted version control
in centralized version control, developers work on a central server directly where as in a distributed version control system

each developer will have a local copy of the central server. In centralized version control systems we will need internet and there is a

chance of single point of failure. whereas in a distributed version control system once we have the repo copy locally we can make changes within
our local machine and parallel development is possible even with out internet (only in cases of push,pull and clone we will need internet)

git concepts:

create folder

make changes in any file(example text file)
git init
git status
git add.

git commit -m "message"

push your changes to repo (remote repo)

git push origin master

create git hib account
create repo with same folder name

git hub token :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 to push into a new repo

git remote add <url> to manage the repo from terminal


git clone < url> ssh or https to copy the projecty to local machine

git fork command to copy entire repo from another user to our repo

GIT branching

feature branch to staging then staging to pre production and then pre production to master branch

git branch --> command to see which branch we are standing (with * and green)
git branch -a shows all the branches in our account.

git branch <branch name> command to create new branch

git checkout <branchname> to jump to another branch

to bring changes from one branch to other go to that branch and raise a pull request. Once a pull request is raised , we can then confirm merge request.

once merge request is confirmed we will get the changes from the newly create merge to the master branch.

git checkout -b <branch_name> will create a new branch and will move the prompt to that branch together.

git branch -d  <branch name > deletes the branch from local machine

git push origin -d <branch name > will delete the branch from the remote account also.

git fetch --> will bring in the new branch from remote repo to local with out actually bringing in the changes

difference between git fetch and git pull is git pull will bring the branch and file changes,,while fetch will only map the new branch but not the changes

git tag -l <give the list of all tags>

git tag <tag_name> creates a new tag

git tag -a <tag name> -m "message" creates anointed tag which is a special tag
git push origin <tag_name> will push the tag to the git hub

the above will create tag on the latest commit. to create a tag on the previous releases,

we will do later tagging

to do it we use below command
git tag <tag_nam> <commit_id first 6 digits>

git tag -d <tag name > will remove tag from local machine
git push origin :ref/tag/tag_name will remove tag from the remote repo also.

git checkout <filename> will undo the changes

to undo previous changes we need to use following commands

git checkout
git revert
git reset

the areas of GIT are working area |(local repo) ---> staging/indexing (after adding )  --->commit (after commiting) ---> git push (after pushing the file to github)

to move files from working area to staging area we use git add . further to move files from staging to commit area we use git commit 

to move files from commit area to git hub we push the code using git push <origin> <master >

git checkout <file name> will revert the changes at the working stage.

git checkout <file name1> <filename2> <filename3> will undo changes of multiple files.

git diff --> will give changes between commit area file and local file  <the changes added will be displayed in green and then changes removed will be shown in red>

to undo changes from commit area back to working directory  or staging area (depending on option )we use git reset command

git reset is of 3 types 

git reset --soft Head~1 ---> this remove the commitn message and bring the file back to staging area from commit .so all we need is to commit the change again.file changes
will still be present inside it.

git reset -- mixed Head~1 --mixed can be skipped and git reset by defualt is --mixed .here the file changes are brought from commit area to working directory
changes will be present in the working directory but we will have to add them and then commit if we want any action on them

git reset --hard Head~1  --> will remove the file from working directory also.

git revert <commitsha> first 6 olr 7 letters of commit id will revert the changes from the git hub all the way upto working directory meaning it will

completely remove all the changes. it will also commit a small change indicating the revert operation.

git commit --amend -m "message name " will change the commit message from a previous commit ---> will only work for the latest message

git rebase -i HEAD~<message number > ..will amend commit messages of  the commit number from the head.

git log--oneline -->will give commit message and then id in a single line

git rebase -i HEAD<5> and then select "S" option for sqaushing..squashing will meld different commits together.


while we raise the pull request we get 3 options git merge ,git squash and merge, git rebase and merge in ui

to see in details

suppose in master branch there are 3 commits , then a new implementation came in a new branch called "feature " and it has 2 commits.


then later on in the master branch if a new hot fix , we need to understand what is the result of it if we use the above 3 commands.

to merge changes between 2 branches within UI, we go to the branch from where we need to merge the changes  and do a pull request. then give base branch 

and destination branch.base and compare in case of github

the problem with git squash and merge is we lose the development history and commit data as it willo merge all the commits into a single message . 

with git merge , even though we get the commit history and development history,the  data will not be linear and feature branch data and master branch data

get jumbled up. so the problem is not completely solved even with this approach

with git rebase command, a clear development history and commit history is obtained. so, it is always the best to use this option if development history
has to be kept intact

git cherry-pick <commitsha> will bring a specific commit from one branch to other.

git stash command stores our working copy temporarily for us to switch branches and then start from the reference poiint again

git stash apply --is to save the working copy
git stash pop -- is to remnove then stashing we need to use stash id in this command.

git stash will only work on the existing files but not new files.

Branching strategy in GIT:

release  --> staging  -->preproduction -->production  

.gitignore file in git environment will add files that have to be avoided for committing


Jenkins:

To install jenkins we need Java jdk 11 first then we have to install jenkins
we can then start jenkins by jenkis service start

jenkins is supported only by jenkins 11

to see java version type java -version.please make sure only java 11 is supported.

In windows, afterinstalling Java 11 edit the environment variables

add envirnoment variable name as: JAVA_HOME and give value as its path .change this in user setting of environment variable.

we will also have to edit the PATH variable by typing %JAVA_HOME%\bin PATH  is a separate environment variable.

to run jenkins in windows we need to type java - jar jenkins.war from the command prompt. It has to be openend from where the jenkins war file

is installed. by default jetty is the server that runs inside jenkins

To start jenkins in browser we use port localhost:8080 .password for jenkins is stored inside a sceret folder in the jenkins the installation

folder path.  

to install jenkins in AMI linux machine :
sudo yum install jenkins -y

to check whether jenkins is running or not in an AMI linux machine we use the command

some basic jobs in jenkins that we see are:

print hello message

pull code from github and work with it --integrating github with jenkins

create sample java project and work on it.

to create a job in jenkins, we first start its service using command jenkins service start.

we open the jenkins server on the port 8080. For this to open we will have to edit our security group policies to enable tcp traffic on 8080 port.

we    go to the browser and type our ec2 instance id and jenkins port number 8080 separated with colon,

the first job we create is a basic hello world job. In this job, we click on create new button and then update our job name .

We also give the command to be run for our job at the bottom section. here we can give echo "hello world" oneimpiortant option to select

is runtime env which we need to select as bash shell. so this can be considered as a very basic job.

second job is to integrate a git repository with jenkins and run it on jenkins. to do this, we should create a new job and then configure our setting

by adding our git repo connection link(used for cloning the git repos give the https link instead of the ssh one).  we also need to give our git hub credential to pull the git changes.

then we have to make sure we install git in our ec2 instance (if not present we need

to install it by using command sudo yum git install -y. Along with configuring our git repo within the job page, we also need to change it in

the main jenkins page by editing manage jenkins page . here it expects the executable path of GIT and if GIT is already installed in our ec2 instance

it automatically pops up there. so it is essential that git is confiugured in these 2 locations mandatorily. p

In Jenkins there is option for us to manage the build trigger automatically. these are of 3 types

they are :

poll scm  ---> here job will be triggered at a specific time if a change is pushed.

build periodically ---> job will be triggered in a period after a specific time even without changes by picking latest change

and 

webhook --> job will be triggered immediately if code changes are made .here there is no waiting period as in the case of poll scm.

all the above 3 jobs are cron jobs. poll scm and build periodically are pull based mechanism and webhook is a push based mechanism.

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 --->github token

to create poll scm in configure page of a jenkins job, in the build triggers section select poll scm and provide the necessary cron job schedule

if the job is for periodical do the same but select build periodically

however to select a webhook there is no need to create a cron job .instead select webhook option in the jenkins job page build trigger section
in the git page of the project select webhook option and copy and paste the jenkins url link followed by /github-webhook.this we will get inside
the specific repo setting but not the overall repo.

Maven:

Every project in Maven has a pom.xml file. (project object module) generally, a maven project may have src,test and pom.xml file.

pom.xml file has syntax . 

it primarily has tags like

dependencies, groupid, artifactid( jar file name), scope (when the jar file is used build or test). we can add multiple dependencies.

example is below:

<!-- https://mvnrepository.com/artifact/org.apache.maven/maven-core -->
<dependency>
    <groupId>org.apache.maven</groupId>
    <artifactId>maven-core</artifactId>
    <version>4.0.0-alpha-5</version>
</dependency>

please note that maven architecture contains different repositories , local,remote and central.

.m2 folder in maven architecture is our local repo. any dependency files that are needed for our file will be downloaded from remote repo and copied

into our local repo. If the dependency is not even available in remote repo, it will be kept in cental repo (which is internet).

the remote repositories example are jfrog,nexus,maven in our own linux server and it can be used by entire company staff to download the repo

but if the artifacts are public they are then downloaded from central repo (internet ).  only custom dependencies or jar files can be kept in our 

remote repo.

in pom.xml file, the code we write will help our project download the data from the internet. from next time onwards it will directly use the data
from remote or local repo

Maven project has following lifecycle:
clean ---> removes previous artifacts
validate --->validates project structure 
compile -->  syntax check 
test ---> executes test classes. 
package ---> final output jars --> they are deployed on to web servers like (apache tom cat )
verify
install
deploy

how will we know our package gives .jar or .war

so in pom.xml file a tag called packaging is present, which gives info about file is .war or .jar

but by default it is .jar 

we use the lifecycle untill package. so we use mvn <lifecycle> for the maven project to work for ex: maven package

in maven package , validate, compile test and package execute together with one single command .. so if we give maven package all the steps will get executed

clean is a default lifecycle. every package step will create a target folder which will have test report folder (final output .war file). when we do target again,

it will remove the previous target folder and create new target folder (default program --> meaning clean). 

use srping3 maven github example (https://github.com/mkyong/spring3-mvc-maven-xml-hello-world) please note all the tags in this link.please understand
that all the dependncies,plugins etc will be added by the developer.

we also have settings.xml in our maven project, in this we will have info of remote repository like server details ex: id,pwd private key,file permission usage of proxies etc.

to create maven project we use the command : mvn archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4 -DinteractiveMode=false

in the above command archetype:generate group id will create maven project and it will name the project according to the artifactid name which is my-app.

Maven -Jenkins , Apache tomcat-Jenkins integration.

Maven -jenkins:

After creating a maven project using above command, we can push the pom.xml to github. we should then create a free style job in jenkins prefarbly with the same name

as in maven project. Later we should integrate our maven project with the jenkins tool. For this purpose, we have to go to the configure page within the job of our
jenkins page and add our maven github url .we can use the credentials of our github .  Along with this, the most important aspect is also to present the maven
build pipeline stage , example : package: in the build steps which help the jenkins understand the step in maven life cycle until which the 
process will be executed. Please note that a .jar file or .war file will be only be created if we give the package command in the build steps section.

Any command representing the lifecycle before package will only execute the operation until that point only.

A successful package step is the only option for us to see a .jar or .war file inside the target folder and in test report folder. jenkins job results are stored in 
/var/lib/jenkins folder.

Now, a jar file can be executed using the command java -jar <jarfilename>.jar.

But how to create a .war file and deploy it in the server. For this purpose, we will need to understand .war file creation from pom.xml file and know about
apache tom cat server also. 

It is to be noted that the creation of. war or.jar is dependent on packaging tag inside the pom.xml file. So, we can mention .war if we need .war files.(web archive files).

For this purpose, we can fork a java spring boot code from an example github like : https://github.com/mkyong/spring3-mvc-maven-xml-hello-world.

After this we should create a free style job named javaspring and can integrate our spring boot github with it by giving its url and credentials.

we should again give the option as "package" in the build setting section. So far, we haven't reached the point of deploying our application on to server.

So, for this we use apache tom cat as decribed above. Apache tom cat can be installed in 2 ways .First can be downloaded directly from the apache software

website and second is through command like sudo yum install apache tomcat -y. Whatever possible manner is used the resultant directory contains three important
folders that have to be understood definitely by all. They are 

bin --> it contains tomcat start and stop scripts . tomcat just like jenkins is a special service and has to be started using command 

sudo service tomcat start , stop , status.


conf ---> folder which contains important .xml files like tomcat-users.xml and server.xml which are responsible for username and password details

and changing the port number of the tomcat service. For username and password details ,they are set by default as admin and adminadmin.however, when we start
the tomcat service and give these credentials, it doesnt work. Reason being in the tomcat-users.xml file they are commented. We have to remove the comments 


we should then open the file server.xml and change the server port to 8081 or 82 any port of our choice. This is to make sure the same port is not in 

conflict with the already existing service (mostly likely jenkins). After we do these steps we have to restart the tomcat server before using it.

Please note that these files are present in the locations mentioned above when we install the tomcat from the website by downloading the zip file.

instead if we install them directly from the command prompt using the sudo yum install command, then they are (atleast server.xml and tomcat-users.xml)

inside the folder /etc/tomcat .After these settings are done, we load the tomcat page using the public ip address and the port number. Just in the case

of any other service even with this we have to edit inbound rules in the AWS security group page by adding 8081 port.

Please note that as in the case of maven and git, we will need a special plugin to integrate tomcat with our jenkins too. It is called as

"Deploy to container " it has to be specially downloaded from available plugins.the input to the plugin is the location of the .war file in the 

Maven project. usually present in target folder. so we need to give itn as **/target/*.war

General  diagram of the flow so far along with the details.

Git(for changing files) -->github (online repo to see our files ) ---> jenkins (for integration purpose){maven(for compilation and artifact creation like 
.war/.jar),apache tomcat (for .war file deploying).

Contiuning with above process, after creating the apache tomcat on an isntance along with Jenkins, try and setup tomcat server on a separate instance.

we can take 2 git hub projects for practice 1) petclinic 2) game of life. the idea is simple. to fork the code in those repos to your github.

integrate github,maven with the jenkins instance of our ec2 instance and build a .war file after which it has to be deployed using tomcat.

some examples where the application may not get deployed in older ways where the process follows like  this  

github --> jenkins  (git hub, maven, tomcat integration), are 1) both tomcat and jenkins seerver running on the same instance so we need to run them separately

2) increasing the heap memory of jenkins instance in the file /var/lib/jenkins (sys/config and jenkins file ) .in this file we need to update the command as below.

JENKINS_JAVA_OPTIONS="-Djava.awt.headless=true -Xmx1024m -XX:MaxPermSize=512m

if the above 2 wont work then we can also go for changing the type of ami instance from t2.micro 2gb t2.micro 8gb (please note that this is chargeable).

-----
Sonatype nexus and Jfrog:

There are repositories in market that help us to keep a copy of of our .war/.jar files which can be reused as and when required ( we can also docker files,helm chartsetc).

both Sonatype and Nexus have 2 flavours ---> oss and pro  OSS is free and Pro is chargeable.

Nexus can also be implemented in 2 types free style pipeline Also for nexus we will need medium instance  (t2.medium). please create instance on ubuntu.

So, the basic job of nexus is to take back up of artifacts and store it in its repository.

To install nexus follow the material . After nexus installation is done and user and group are added according to the document,

we need to add nexus service as part of the linux bootup. we need to add a softlink for nexus folder that we created to the init.d of etc folder.

in the nexus folder, we change the file called nexus.rc which is inside the bin folder and we add user there. Please note that the default
port in which our nexus service runs is 8081.So,we have to edit our instance's security group to enable this service.

once the port is up and running we can open nexus repo in our server and create a sample repo for our purpose.After that, to copy the artifacts
we integrate the same through jenkins.

For this purpose, we need to install plugin for nexus to integrate with jenkins. which in "nexus platform". After this is done we need to check for
nexus repository manager publisher option in build steps . please note that maven goal and nexus publisher are present in build steps where as deploy to container

for tomcat is present in post build steps. so we select nexus publisher manager option here. Also, we have to add credentials for nexus in the manage

jenkins section of the main dashboard also. We should give nexus url, add username and password and apply.

After adding the credentials in the main jenkins dashboard, we should come back to configuring our job where we added the fields for build options
please tc  that the .war file that we will give should have full path.Also, the build version number can be kept generic as  ${BUILD_NUMBER}.

Alos, this is helpful to automatically trigger build once there is an update on the version number . 

Please note a basic difference between free style and pipeline jobs is configuring the fields of the tags from pom.xml file has to be done manually everytime

there is a change in the pom.xml in free style jobs.where as pipeline jobs can handle multiple complexities on its own.

After triggering the build and once that is successful , our projects artifacts get uploaded on to the nexus repository.



Sonarqube:

<Space left for sonarqube>







-------------------

Downstream and Updstream Jobs : By adding the option Build after other projects are built in Build triggers section of a job and specifying the Job name

we can add the upstream job Similarly  In post build actions , we can add the job name in Build other projects to trigger a downstream job.

Junit test report:

We can see the results of the job in a GUI way on the main page of the Jenkins page by setting up Run JUNIT tests in post build actions.

Here we need to give the input as .xml file present in the surefire folder of the workspace directory.
------

Dashboard UI: In case of seeing details related to multiple jobs at one place, we make use of a plugin called Dashboard. This will help us see the 

details of various jobs in a much better graphic presentable format. This is present after we click + symbol just beside main page .We should then
add the details accordingly.

------------

Msster-Slave architecture:
There is always danger of loading the main Jenkins server by running all jobs in it. Instead of that we can make use of slave servers Slave servers
run jobs inside them and provide results . Slaves will remain active as long as the master is alive.We also have to launch slave as soon as it is down

To configure Master-Slave architecture, we will have to click managejenkins-->Nodes-->+ New Node -->Node name and select permanent agent .

After this please understand a very imoportant field knownn as Number of executors. This indicates the total number of jobs that can be run on a 

server. This generally depends on the number of CPU cores. We also have to give the root remote directory (create an ec2 isntance andn create a new folder

inside it) this is the folder where our work will be stored.Labels/Usage options are staright fwd(can keep use this node as much as possible)
In the launch method,  try and select launch by ssh agent.

We then have to give credentials ,Host (Host is EC2 instance public IP address) and  user name is ec2-user. After that we also have to copy and paste 

our .pem key from the place where we download it during instance creation. we can then save and apply. The master node is called as Built-In Node and the slave 

node will be below it. Now the most important thing is to configure the slave node in the Built in node job. For this purpose, in the General section  of the

jenkins main job page, we select "Restrict where this project can be run" option and give our slave job ID/Name. With this setting enabled, any triggers on the

main job will be redirected to the slave jobs.

Pipeline Jobs vs Freestyle Jobs: 

All this while we have been discussing about the free style jobs. Freestyle jobs carry an issue of not being able to dynamically

update the content of pom.xml or parse json or yaml files on its own.For example, everytime there is an update related to a project

and if maven triggeres a new .war file, it is not possible with freestyle jobs to update them In such scnearios we use the pipeline jobs.

We use something called DSL (Domain specific langauge) to write a code to perform pipeline jobs

One example of DSL is groovy.This is called as pipeline as code.But please remember that this is used only when even pipeline syntax cant be 

implemented.But generally, we use pipeline code with pipeline syntax.

Pipeline jobs run with a syntax.

Pipeline code also has 2 types of syntax:

1) Scriptbased pipleline -->old ,slow,takes more executors
2) Declarative -->new, fast, less executors, clear readble

Script based code looks like below

node{
stage(){
}
stage(){
}
stage(){
}
-----

Declarative syntax looks like :
pipeline{
agent any;
stages{
	stage('clone'){
		steps{
			----
			}
		}
	stage('build'){
		steps{
			----
			}
		}
	stage('deploy'
	 ){
		steps{
			----
			}
		}
	}
}

There are again 2 ways to write this code 1) pipeline script 2) Groovy sandbox

This is seen after we create a new job and this time selecting pipeline instead of freestyle

It can be seen that with this method we will get only 3 options as opposed to 6 in freestyle

These options are General,Advanced project and Pipeline.

Then we click pipeline--> we select pipeline script in definiton and then uncheck grrovy sandbox at the bottom.

Pipeline script from SCM is also similar but instead of writing directly on the UI we write the code in a jenkins file and upload it to 

our GIT We keep that GIT link the project configuration.


How to write multiple jobs at a time.. We get pipeline syntax(basic) at the configuration page.Once we click that we get a snippet generator

We can use the tab beside the snippet generator sample step and from that select build:Build a Job option.We also have to give the project/jobs
to be built and then at the bottom click on Generate pipeline script.This will automatically generate a syntax that can be copied and pasted in the 
script section of the pipeline option in the pipeline job that we created.After this if we trigger build on the pipeline job, it will internally
trigger all the jobs that are part of the script section in the pipeline option of our job.

In this way we can trigger builds  on steps related to GIT cloning, Maven war file generation, Deployig this .war file in to tomcat amnd also .

For this example in created, Pipeline pipeline_project_complete in jenkins and wrote the pipeline code for it.

copying this .war file into a nexus repository.In pipeline jobs, we dont see "workspace" folder in the UI as we get it in the Free style jobs

So, for this purpose we have to make use of the step "Archive srtifacts" within the pipeline code in pipeline syntax section.

From this we select the archive artifacts option and then we also select the files to archive and put the content in it as /target/*.war and then we click

generate syntax just in case of other steps in the pipeline syntax generattion. we get the syntax as "archiveArtifacts artifacts: 'target/*.war', followSymlinks: false"

We take this syntax and we add a new step in our original pipeline code and we name it artifact (stage) and in the steps section we copy this code.Please 

remember to follow the flow correctly i.e first step is to clone the build, second is to make .war file that is use maven and then artifact the .war file.

After artifact step is done , we have to deploy the .war file in to tomcat server. For this purpose we use curl command.

curl command contains options like -v and -t in our command which represent verbose and upload.

curl -v -u credentials like admin:admin -T <source> <dest> here <source> represents the .war file location in our jenkins server and dest is the ec2 instance where our tom cat is deployed.

so the actual command in our case looks like

sh "curl -v -u admin:admin -T /var/lib/jenkins/workspace/pipeline_project_complete/target/*.war 'http://http://13.52.99.80:8080//manager/html/deploy?path=/pipeline_maven_application&update=true'"

here curl means copy url 

-v verbose --for checking logs in details
-u -- command to take username and pwd

admin:admin credentials for tomcat
-T option informing we are uploading

/var/lib/jenkins --represents the source folder

https://ipaddress and path="" & update=true is the actual destination folder with context path which is the path in which we will upload our. war file

&update =True is option to continue to deploy the changes on to tomcat even when there is no real change in the build.If we do not put this option,

whenever we rerun the above command, it fails as there is nothing to be deployed and its already been taken care.

With the above approach there is an apparent issue  as we are exposing our credentials in this method. So , we have to make sure our credentials are not exposed.

For this purpose we add our Tomcat credentials in the manage jenkins page .Then inside our pipeline job, we create a new pipeline syntax by selecting the option

withcredentials. In this option we select cojoined with username and pwd and give our tomcat ID that we give in the manage jenkins page.

When we generate the pipeline syntax, we get the required output.In our case, its as follows.

withCredentials([usernameColonPassword(credentialsId: 'remotetomcat', variable: 'tomcat_credentials')]) {
    // some block
}

We carefully then copy this content below the deploy step and on top of existing code. We also, remove the admin:admin part of the code from the 

current code so that credentials are not exposed and just give a variable syntax like ${<variable name} Usually the ID which we give in the manage jenkins page.


To read pom.xml file in our pipeline syntax ,the plugin that we need to use is "pipeline utility". Within this plugin we use the API named readMavenPom:"pom.xml"

Also, we use findfiles method get the contect from the above pom.xml.The code can fetch the path of artifact, packaging name, id etC.

After that, if the artifacts are present we upload them to nexus . For this we check the code from page upload artifacts to nexus repository.

The plugin  for this "nexusArtifactuploader".The code looks like below.\

nexusArtifactUploader(
        nexusVersion: 'nexus3',
        protocol: 'http',
        nexusUrl: 'my.nexus.address',
        groupId: 'com.example',
        version: version,
        repository: 'RepositoryName',
        credentialsId: 'CredentialsId',
        artifacts: [
            [artifactId: projectName,
             classifier: '',
             file: 'my-service-' + version + '.jar',
             type: 'jar']
        ]
     )



Parameterized builds:

In jenkins, we have an option where we can pass inputs to the jenkins job before we make a build. These inputs can be string,branch and tags 

Branch and tags are direct concepts from GIT and represent the same whereas strings can be name of the server on which we might want to deploy

To check this with example we create a free style project and we select "This project is parameterized" option  and then select String Parameter first

After which we give parametername,value and its description. In case of string based parameters we can give inputs as Dev/Test/Prod. So we can begin with Dev.

Also, please note that while triggering build , we will see the option build with parameters instead of just build now as in the case of normal builds.

In this way, we can also add test and prod servers .this is particularly useful whenever we have a condition about what type of server has to run 

what type of build.

In more real world the deploying of applications may happen on tag and branches of GIT hub.So, we create a new project and give it the name branch

based deployment. But, for this , in the option "This project is parameterized" we dont have an option to add parameter related to GIT.For this 

we will have to install plugin related to it. The plugin name is " git parameter" plugin.After installing it we will get the option in parameterized build
section.

We then have to continue and update some important information like Name,Description,Parameter type ---TAG if tag case,Default Value ---> tag value from the 

git repo which we want to deploy in our case it was v1.0 from URL :https://github.com/shashanksrinivasa9/first.  

Then in the SCM section we need to update GIT URL , its credentials and also the branch from where we are trying to pick the tag.

In general for tags this can be updated as refs/tags/${tag}. With this only that tag's code will be downloaded.Once we set this up and click

on build with parameters we will see all tags available in our git hub and we can select our preferred tag. We can cross check that with 

files in our workspace of jenkins server and the files in the git hub repo. 

We can also use Branch in place of Tag and deploy it on to our jenkins.In this case, we use git parameter as "branch" provide the default value

which in our case is Master and then the branch value to be picked up as ${branch} We can then select the branch from which we need the code and it 

will be downloaded.


Git Hub actions :

Git Hub actions is CI/CD tool from GIT . GIT Hub gives the user a hosted zone  called as "Runner". But for every job git hub actions

give the user a new runner or server.So, if we want to create a simple job push some data, script we can use runners.

To start a job in github actions click on actions in git hub and then click set up workflow for yourself.

It opens up a .yaml file. where we can type the code for running the job.

the main syntax looks like below. we can check the syntax in github sybntax workflow page for details.

name:"job name"  --> project name

on:  ---> this will trigger job on only when
	push: --->somebody does a push operation 
	branches: <branch_name> -->on this branch
	pull: --->does a pull request 
	branches: <branch_name> ---> on this branch.
jobs: --->this depicts the part of the workflow which is doing a task. for example making the build.
	builds:--> this is where build happens
		runs-on:--> <server name> the runner where the build is run which in our case ubuntu-latest.
	steps:--> this represents the sequence of steps in the process
		uses: actions/checkout@v1  --> line to tell the process to check out the code from the given branch.
		- name: Run a one-line script ---> this is the line to represent the one line scripts like printing or giving any other linux command .this always have 2 fileds name,run.name represents the name of the step and run executes the step.
		run: echo Hello, world!
		-name:java version
		run: echo "java --version"

Sample script that i picked up from jmstech git hub  https://github.com/jmstechhome/jmsth21_first.git  is as follows.

name: CI

on: [push]

jobs:
  builds:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v1
    - name: Run a one-line script
      run: echo Hello, world!
    - name: java version
	
	
	name: web deployment

on: [push]

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v1
    - name: Set up JDK 1.8
      uses:ctions/setup-java@v1
      with:
        java-version: 1.8
    - name: Build with Maven
      run: mvn -B package --file pom.xml
    - name: deploy tomcat
      run: curl -v -u ${{ secrets.TOMCAT_USER_NAME }}:${{ secrets.TOMCAT_PASS }} -T /home/runner/work/spring3-mvc-maven-xml-hello-world/spring3-mvc-maven-xml-hello-world/target/spring3-mvc-maven-xml-hello-world-3.0-SNAPSHOT.war 'http://ec2-13-233-173-136.ap-south-1.compute.amazonaws.com:8080/manager/text/deploy?path=/myweb_dev_action&update=true'

	
We have to write this script and commit it on master branch. After that when we see the actions tab again we will notice that the job would

have run completing all the steps .

The default git hub hosted runner will have some default softwares for java,maven,docker.

If it is not present we can install them.

But,instead of default runners we can also use our own runners as with default runners we cant do ssh to them and we cant store session and store data

with in built servers. So, to do these things we use inbuilt servers which are called as self hosted runners.

code-->compile-->validate-->test-->package-->deploy . this is the general CI/CD flow that we should check in github with self hosr runner.

To connect a self hosted runner to github actionswecreate an ec2 instance  go to the repo-->setting-->actions-->runners and new self hosted runner

and then select the OS and architecture as x 64 we want the job to run on in our case linux. then there are commands to be followed and copied in to our instance.

After that the commands to run github actions on selfhosted runners is same as above which runs on ubuntu-latest.

The difference lies with run on option where inn the case of our own runners we use self-hoested instead of ubuntu-latest.

Along, with deploying our .war file in maven, we can also use nexus to copy the .war file in to nexus repository.

Docker:

Process before docker 

If we want to run a java applicaton on multiple server there are some preqreusisites

We have to install O.S ,Java,Maven,Tomcat and other than this dependencies. Also, it takes 1 gb RAM 1cpu Core and 15gb hard disk for a 50 MBapplication.

So, a lot of space is wasted. Also at every downtime, we have to redo everything. Also, we take lot of time to do these things.

To remove all these problems, we use Docker, this will take less time and uses less resources as compared to the older technique.

A typical docker ubuntu container takes nearly 70-75 mb. Alpine linux another version uses even lesser space about 5 mb. And the installation 

completes with 1 command.

A major point to understand is difference between Image and container.


Image is like switched off mobile and Container is live phone. We have to run image to create a container .Application runs on container.

Container generally means an application. Containers run on  linux machine.Multiple containers can be run on a linux server.

Whenever a container goes down immediately an new container is created by container orchestration tools like K8s.

So, deployment, maintainence and working on containers has become easy by reducing the resource and time to deployment.

What is the difference between monolithic and micro services?.

A monolithic application entire application  (.jar file, db and an application) is deployed in a single place . Its like MVC (model,view controller)

model. Everything will deployed at a sinlge place.But this had obvious issues.

Like if  "x" developers code is working and "y" developer programme doesnt then there is an issue with this kind of approach.

So, the industry started for another alternative. 

Take example of a travel website (searching,booking,cancellation, payment) these modules can be developed by different developers individually.

In this model, each developer deploys their own .jar file.


Each service communicateswith on anothert using "RESTFUL" APIs.Agood example is transaction of ATM between 2 different banks.

RESTFUL service provides tools or ms framework like (spring boot,ms languages) 

This kind of job can be done using micros services just like the travelling example.So the dependencies are automatically removed .So, application
deployment becomes fast.

More reliastice example of Docker.Is the example of transferring things from one city to another. As we keep things in one big container, similarly

all the services will be put in to a container.

Docker slogan is Build-->Ship-->Run.

A docker file (with commands) is built in to Image. The image is stored in a repo by pushing it  (docker hub). The same image can be stored in multiple places

like ECR,Nexus,Git hub The image is then pulled to run the container.This container acts like our application. We can run the application for our purpose

Difference between virtual machine and Docker:

What is a virtual machine?

If we want to install ubuntu on top of windows we have to install VMs. To install VMs we need to install Hypervisor in turn.The process and space the 

computer uses when hypervisor is in use is large.

Generally whenever we use a virtual machine we will see the following hardware/software combo  (from top to bottom)

hardware -->windows -->hypervisor--->(guestos-->lib-->app),repettion of guestos lib and apps.

Where as in case of Docker what we do is we have hardwar --> guestos-->docker daemon-->(app,lib)

In docker we dont need to install hypervysoe,we dont need to install guest os and libraries are also shared between apps (if they are using the same app)

This is acheived by writing the docker file which has details about OS and command to install the reqiured softwares like run java and run maven.

The docker architecture mainly consists of three parts

Docker CLI--> Where we run the docker commands

Docker daemon --> the exact docker container or software where the process is run (container)

Docker hub --> Repo where docker images are copied after buidling
Docker build, Docker push , Docker pull are some of the basic commands to do these tasks.

Docker CLI is installed automatically. 

Image is created after we build an image. Container will be up and running only after we run the image.

So lets get started with docker by creating a new ec2 instance.

We give the command sudo yum install docker

docker -v--> is the command to check docker version or we can also use
docker version for more further details. 
20.10.23 --> is the version installed in my machine.Docker is implemented in go language.

sudo service docker status --> it is the command to check the status of docker daemon.

This completed jenkins with docker freestyle job example
Now lets take a look @ jenkins pipeline with docker.


we can start the docker service by typing the command  sudo service docker start.

sudo docker info --> this command gives detailed information about the docker server with number of images/containers running/stopped/paused etc.

please note that in every command we used so far we had to type the "sudo" command. To avoid this problem we have to enable special

permnissions to ec2-user using commands.So,basically we have to give sudo access to ec2-user. Basically to a user access to any group

the command is

sudo usermod -aG <group_name> <user_name>

so, in our case this will be sudo usermod -aG docker ec2-user.But wehave to restart the terminal of ec2 to make this effective.

docker images -->command to give the list of images in the system.

docker search <image_name> --> to search imaged from dockerhub. --> this will fetch a list of the same image from the hub. we can check

whether they are official image or not and also see the top one with more stars.

Everyn public image is stored in the dockerbub. Our docker knows the docker hub path from the url we get in the docker info command

docker pull <image name> --> command to pull the images from docker hub. for example  docker pull ubuntu---> this will pull the latest

docker inage.

if we want to downaload a speicific version of image we can type as docker pull <image_name>:<image_version>

for example docker pull ubuntu:21.04

Generally, in real time, "alpine" is the linux software is used .This is a light weight linux software. only about 5 mb image

very light weight containers which we cant login but can be used to do simple basic appication with less size are called "Scratch"
very light weight containers which we cant login but can be used to do simple basic appication with less size are called "Scratch"
docker ps --> is the command to see the running containers.

docker run -it <image_name> ---> will run the container (this will help the app run in an  interactive way  on the terminal we are using).

check example docker run -it jpetazzo/clock ---> to see the time in UTC on the termninal we are running.

to detach the container from terminal and make it run in the background we use ctrl+p+q command

to check how many containers are running in the system we type docker ps

docker run -d <container name> will run the container in the background and not in an interactive manner.

when we type docker ps and see the list of containers running, we see the names of the containers too. here by default,

container name is assgined by dokcer. if we want to change the docker name to name of our choice we can type the following command.


docker run -d or -it --name <yourname> <image name>

example docker run -d --name clock jpetazzo/clock ---> this will give a new container running in the background with your container name.

docker logs<container_id> will give the log details of a particular container. but this will give a lot of information

to check live logs we can use docker logs -f --tail <number of logs> <container_id>

for example we can use like docker logs -f --tail 1 <container_id> we can also use container name too just like 

docker logs -f --tail 1 container name.

To remove running containers, we need to first stop them and remove them.

the command is 

docker stop <container id>

example docker stop efd5a1f414d1 --> One thing to note with this is that stopped containers are not shown in docker -ps command

so, we have to use the command docker ps -a --> to see both stopped and running containers together. "a" represents all.

to check only stopped containers we can use grep command from linux . We can then search it with its status "Exited" string to see the stopped containers.

docker ps -a |grep Exited.

to check only running or stopped container IDs, i mean with docker ps -a ---> along with ids we see other info  like time when created status etc etc

we can use the command docker ps -q or  docker ps -qa

to remove docker after stopping we can use docker -rm <container id>

but to remove docker which is running we have to do it forcefully by the command docker rm -f <container_id>.

Please note that removing a container wont remove its corresponding image.Image continues to stay until we remove it.

Now, to remove the image also we need to use docker rmi <image_id> or<image_name>. But,please note that if we try to remove an image 

that is being used by a container,it throws error and informs us that the image is being used by a specific container.

Please note that when we try to run a container using the command docker run -it --name my_name <image_name>

example: docker run -it --name my_container ubuntu ..this will take us in to the terminal of that container but not our ec2 instance.

Pressing exit or ctrl +c will stop the containers, so if we want the containers to detach and run in the background always type

ctrl+p+q. Always remember stopping is different to completely removing in docker terminology

Another important point to note is , whenever we install a software inside a container using an image that software is specific to that container itself

This means aniother container which is run using the same image will not be having this software. Please make use if "figlet" software to make it 

understand better for yourself.

Once a software is installed in a container, and the container is stopped and removed all the softwares installed in that container are removed.

So, we cant use those software in that container when we create a new spin.

So, if we want to reuse the softwares from previous containers after stopping, we have to create a new image on top of the old container before removing

This new image can be used to run a new container.

so for this we use the commend docker commit. the command looks like

docker commit -m "message" <source_container_id from where we look to copy image> <new_image_name>

example docker commit -m "figlet software" xxxxxxx <source container id where figlet software is present> <new_image name>

then we can run that image and create a new container.The new container will automatically have the figlet software as it was developed from

the software having it.

To go inside a running container the command is 

docker exec -it <contianer_id> <bash> this is particularly useful if we want to perform any debugging .This is very important

we can use 2 docker commands toghether also.

for example, we can use like docker rm -f {docker ps -qa} ---> this will forcefully remove all the containers.


So,think of the case where we have to deploy web applications usinfg docker container.

We will see both Java console app and web app (both from existing docker hubs)

As we know, to download an existing image from dockerhub we need to to type docker pull <image_name>

We can search the same from CLI and dockerhub too. In docker hub we can see commands about how to run that particular image.

Take example of hello-world image. Pull it and run it and see the outoput.Since this is a console application after its output is displayed

on the screen it is automatically stopped as there wont be any process inisde it to run.

Once a container is created, we cant use the same name to another container by having the same name. It either has to be removed or we have to give 

our new container a new name.
 
Take use of web application tutum/hello-world also.. We pull it and when we try to run the container we hit an issue with opening the web app on the 

port it is being served directly. We can't directly use the port on which the webapp is served and open it directly .We cant do this as the port is 

not published.So, we have to first publish the port before we can open the web app

so, to publish a port for any web app the command is

docker run -it -p <port number> --name "name of the container" <image name>

example : docker run -it -p 80 --name mywebapp1 tututm/hello-world. By this command, container gets in to running mode and when we see docker ps -a

we will observe that our web app container port "80" is internally assigned to a host port .We can then use our ec2 instance public ip and host

port together to open the application. But please make sure to edit our ec2 security group rules for the new host port before we actually try to open

the webapp. But this is a random port that docker has assigned to us. If we want to publish our own port we can do that also by giving the below command

docker run -it -p <your port number>:<container port> --name "name of container" "image of the container"

example: docker run -it or -d -p 80:80 --name webapp3 tutum/hello-world. In this way, we can open the web app using our own published port .

This configuration of publishing the host port and container port for the usage of opening web app is called "port forwarding"

Please note that once a port is used by a service within our machine we cant use it to any other service. We have to use a separate port number

for the new service.However, if we still want to use the same port we need to use a mechanism called "reverse proxy" (like nginx)

How to push our images to Docker hub?. (Our custom image)

We can push using docker push <image _name>

But, we also need to mention the credentials of the docker accoutn where we have to push. So, lets take an example of copying an already existing

image in docker hub like figlet_image and if we want to copy this image and create a new image on top of it we can use the command
docker tag <source image> <destination hub>

for example:

docker tag figlet_image shashankkolipaka/figlet_image_copy:v1 (v1) represents version

the exact syntax  used is this: docker tag local-image:tagname new-repo:tagname

Once we create this new image, to push this image in to docker hub we need to give docker credentials

for this purpose we use the command docker login command

once we type docker login command it asks for username and credentials which are (shashankkolipaka and pwd) for my docker hub account.

There is one authentication created in a json file . 

Once we login to the docker hub from the command prompt we can then push the docker images in to the docker hub

the command for pushing is

docker push <dockerhub account name>/image name:(version)--->

example docker push shashankkolipaka/imagename:v1.

Now one command that is missing is creating an image from scratch.

For this we need to understand about custom docker file.

the command that is needed is docker build for this .

What is a docker file?

A plain text file that has set of instructions to build an image.

It contains info about OS,Software,files,command to run the application etc.

the basice commads that are used in docker file are

FROM ---> (Command to indicate from where the image has to be built).

LABEL: info about the author who created the image (optional)

Working Directory: (fo changing paths like cd )

ENV:  setting up variable while running.

ARG: to pass argument while building

RUN: installing softwares or commands while building

EXPOSE: to understand which port our appl using

USER : to understand which user is using:

HEALTHCHECK : to see the health of the container.

ONBUILD : Used in child containers

ADD: Copy files to docker image

VOLUME: Persistant storage.

COPY: Copy files to docker image.
CMD: Final execution

ENTRYPOINT: Final execution (both CMD and ENTRY POINT are used for making the APP up and RUN).

Take example of Java hello world console application . Clone the java hello world repo

Now think the pre requisities need to run a javaapplication . The things we need are 


install java
clone the code
javac helloworld.java
java helloworld

So,the above steps have to be converted into docker file using below commands.

Dockerfile:

FROM openjdk:8
COPY Helloworld.java Helloworld.java (Copying the file from current path of the file in ec2 isntance to container. we can also use working driectory or 

specify the path if we want the file to go into a particular path inside the container path we can also use copy .. to copyall files from current 

path to container path).

RUN javac Helloworld.java (this is exectued while building the image . This means compilation is done here).

CMD Helloworld --> command to run the application.

once we writhe the above commands into a Dockerfile, we have to build that image using the following command.

docker build -t "image_name:v" . (notice the "." at the end it denotes the current context path of the docker)

so example is docker build -t my_custom_java_app:v1 .

After this command, we can run the image to setupo the container.

docker run -it --name "name" <image_name>

Please note that if there are any changes in the git repo we have to pull the changes to repo again and then run the docker build command again.

Now, after working on a console level application let see about a web application deployment using Docker .

I used spring 3 ecxample. The steps remain the same. Clone the code. Run MVN package command in the command prompt itself.After .war file is 

generated copy it in to the local host to apcahe tom cat server ande deploy it.For this wee need to write the docker file.

But,there is some issue with maven compiler plugin. So i was nt able to generate target folder.

So, the output (.war file was missing). But, generation of .war file using the Docker file from above example causes the size of application inside container to be around 526 mb.

To reduce the size of that particular app size in container we can use light weight images.

FROM tomcat: <tag>

WORKDIR /usr/local/tomcat/webapps  (this is the folder where our .war file is stored).-So, the control jumps to this location. If it doesnt exist it 

creates and goes there

However, if we dont want to use the command WORKDIR we can manually copy the spring.war using COPY command as shown below

then the command is 

COPY <Filename.war> /usr/local/tomcat/webapps/<Filename.war> ---> web apps is the folder inside tomcat where our tomcat holds the .war files.

Finally CMD ENTRYPOINT is needed .But in this case it is now required as it will be picked up from base image. Basically it will check the 

commands in the base image file and start the catalina.sh which in turn will start tomcat.

Finally, as we dont have credentials to open tomcat,we open the app using its public ip:context path (.war file name)

The above example created a containers of size around 700 mB for a small javaapplication.

So, to reduce the size of java app we need to optimize the dockerfile.

Please note that each line in a docker file is a layer. When we build a docker file, everyline is build individually.In this process, docker doesnt 

build the file newly everytime. Instead if the same command is written in some other docker file, it is used a "cache" operation by using the ID of the 

command. So, the layers that are already formed previously are used instead of creating a newe layer

Sometimes when we do the docker build we wont be able to see our changes, we can try and use --nocache=true option while building the docker image.

example docker build <image_name> . --nocache=true.By, default this option is false.

1)Please note that while writing a Dockerfile we can mention the option 'Alpine' in the first command FROM <Image_name>:Alpine
`
This is using a light weight image. This is the first step.

2)We can use && for multiple commands ---> for example if we have 2 continuous RUN commands we can club them together using  && commands

we can also use next line command \ instead of && also for making multiple commands run together.

3) Avoid unwanted copying of files. Remove unwanted files like tar aor zip files once work is done

4) Multi stage  builds--> We have multiple FROM commands in single Docker file. O/p  of first RUN  command will be taken as i/p to the next RUN 

command. 

Note that we also have WORKDIR command in a docker file.  What is the use of thisF

For example, if we have command like docker exec  -it "container id"  of tomcat image  --> we will observe that the afer logging in

the control is not a general ec2-user instead we observe that /usr/local/tomcat is where the command prompt is openend.
This is due to the reason that inside the dockerfile of that base image ,/usr/local/tomcat is mentioned as the WORKDIR.

This is mainly useful when we want to copy files from source to destination. Automatically the required file from our set path is copied 

to destination.

Different OS versions have different Package manager.Package manager is command used to install softwares in the machine .Package managers and OS version

combinations are given below.

Centos --> YUM

Ubuntu  --> APT

Python -->Pip (python inline package)

alpine --> apk
 
node js  --> npm (node js package manager)

So, to install any packages within python/nodejs we need to install pip and npm (respectively). But, for ubuntu,centos,apline we dont need any 

pacakage managers because they are at O.S level.


Node JS application deployment.

So,first we begin with ouyr own nodesjs base image instead of the ones present in docker hub.

To deploy an app using css/nodejs, we need to understand the dependency softwares that have to be installed.

For this purpose, we need to install nodejs ,npm, and a https-server.All these 3 should be written in to a dockerfile.

So, we need commands for the working with the above 3 dependencies.

so the dockerfile should look like 

FROM ubuntu
LABEL skolipaka221@gmail.com
WORKDIR /USR/APPS/Docker
RUN apt-get -y update 
RUN apt-get install -y nodejs
RUN apt-get install -y npm

Instead of the above multiple lines, we can write them all into a single layer using && and \

FROM ubuntu
LABEL skolipaka221@gmail.com
WORKDIR /USR/APPS/Docker 
RUN apt-get -y update \
&&apt-get install -y nodejs \
&&apt-get install -y npm \
&&npm install -g http-server
ADD . .
EXPOSE 8080
CMD["http-server","-s"]

Then clone the repo https://github.com/ybmadhu/web_login_automation . Change the Dockefile inside it and replace it with the above commands

After that build the image with the command docker build -t "image_name":v1 .

It will take some time in the first instance for the image to get built.

then we can run the container using the image that we built. The command for it is 

docker run -it --name "your container name"  -p 8080:<container_port> "image name"

We were able to see node js application deployed and running and we will observe username and password login.(Please note that we should enable

port 8081 in our aws security group ).


Lets see instrucntion 

RUN ADD COPY CMD ENTRYPOINT.


ADD and COPY  --> the purpose of these commands is to copy files from host machine to docker images and then finally to container

But what is then exact difference between the 2

COPY: Copy can only copy from host machine to docker image

ADD: add can copy from host machine and remote urls also to docker image. 

Additionally, the main point is if we have tar file to docker image "ADD" will copy the file, extract the tar file and remove it

whereas "COPY" will only add the files from host machine to docker image but, it wont untar or remove the tar file after extraction.

If we use "COPY" we should untar and then delete the file also. Please remember, "ADD" will do all 3 the tasks (copying, untar and removing)

only when the file is in host machine,If the file is present in the remote URL it acts exactly like COPY .

So, to check practically, create 2 files sample.txt and example.txt. Then use COPY and ADD for each of them and move them to any temp folder .

We can write a Dockerfile to see this operation. The base image that can be used in BUSYBOX.
= 
FROM busybox
ADD sample.txt /tmp
COPY example.txt /tmp
CMD ['sh']

After writing the above Dockerfile, if we build it and run it we will see that shell is openend as CMD instruction does it and inside /tmp folder,
our 2 files are present. This is example of ADD and COPY from from host machine to docker image/container

Sourcecan file/directry in COPY but in ADD we can use when we have tar file or if the source is URL

Lets ee ann example with tar file

command to create a tar file is tar -cvzf <tar_file_name> ./
FROM busybox
ADD tar_example.gz /temp
CMD ["sh"]

this will directly copy ,extract and delete the tar file. However, if we use copy it only Copies the .gz file but not untar it.

Then we have to untar it and remove it within the dockerfile command.

Now lets look at the following commands.''[
\]
RUN ---> Run is mainly used during Build time (when we do docker build this is executed.)

CMD , ENTRY POINT: --> (when we do the docker run this commands are executed).the command that is basically

expected to run after a container gets into running mode is kept here.

All the above 3 have 2 forms Shell and Exe.

Shell form --> direct commands like RUN apt install git --> general format.

EXE form--> RUN['apt','install','git'] --> this is used if we want to give any parameters to our command .

We will see braces for exe form whereas shell form doesnt have any braces.

To try out a basic example containing RUN,CMD and Entrypoint all together i used a sample docker file to download python

In the docker file both CMD and Entrypoint are written wantedly to check which one is executed first. We will observe that when we have

both CMD and ENTRYPOINT priority is given to ENTRYPOINT. 

In the same way if we have multiple CMDs in the same dockerfile --> then the priority is given to the last CMD.

Multiple CMD commands are allowed but not multiple ENTRYPOINT in a dockerfile.

Between CMD and Entrypoint a major difference is CMD allows parameters to be overridden whereas with Entrypoint this is not possible.

for more details on RUN, CMD, Entrypoint please check https://www.baeldung.com/ops/dockerfile-run-cmd-entrypoint

for example:

FROM ubuntu

CMD echo "hello world" --> when this is run , we will see output helloworld and we also observe bash command we can give bash and then helloworld is

overridden with that command from bash Where as if we use ENTRYPOINT this is not possible

Lets now see Onbuild and Health check ..

ONBUILD :

When we have a dockerfile and inside it if we have RUN command  with an echo statement and CMD command for starting bash shell

Now, when we use ONBUILD infront of these commands, they wont be executed when the docker file is run.

But, instead they are run inside a child dockerfile,before its own commands (child docker file)  are executed .

So, what is the usage of executing commands like this. This is particularly useful when we are trying to run multiple dockerfiles

which have a common code, we dont have to repeat the same steps in all the dockerfiles. Instead we can write the common code inside a parent docker file

and we can use that parent dockerfile in all the child dockerfile..But, we have to use the parent dockerfile as base image for the child docker files.

So, please note that the Docker build command executes before any commands inside the child docker 

Healthcheck command:

Consider if a container is running.Hown will we know whether our container is healthy (meaning our APP is running or not).

We can check by opening our app and seeing if it is up. This will work if we have just one app. But,what to do if we have 100s of them.

For this purpose, weuse Healthcheck in our Dockerfile.

So, to do this practically, we create a folder named health check move into int and write a dockerfile.

We can then write a dokerfile. In the dokerfile we keep requirements.txt and put flask framework for pip installation on this.

After this app.py--> python file for just doing a hello world.

Flask framework runs on port 5000.

Then create a docker image and run it. 

In docker run command  generally we give docker run --it --name "name of container" -p port image:v1

but, with the above option , once a container is in stopped, we cant use the container name again. To make use of the container name  once it is 

stopped, we use the option docker run --rm  --name :name of the container -p image of the docker:v1

we have a command to remove all the stopped containers and images from printing on the command prompt.

The command for that is docker system prune -a.

So, when we use instruction like HEALTHCHECK CMD curl <url> --> this instuction runs as part of  CMD command during Running step.

Once we build and run the image, and see docker ps -a , we can see the container is running and along with it we will also get to know its health 

status. So, we dont have to open the application on the browser again.

Expose Command:

To make users understand on which default port our application is running on , we can use EXPOSE command in our dockerfile.

Assignment:

A java jar file is given. We need to write a docker file, using that jar file. It runs an application which needs a property file (we will mention username,

pwd, url) So, the property file and the jar file have to be copied in the docker file and once we run that docker file,the java app shoukld run.

We need to create 2 containers one each for app and database.

ARG vs ENV

ARG is build time variable and ENV is run time variable.

We can overwrite ARG and ENV variables accordingly. That means variables set with ARG canb be overriden during build time. Whereas variables

set with ENV can be overriden during the RUN time.

Please note that the first instruction in a Dockerfile is always "FROM". On top of "FROM" we can pass one special instruction i.e, ARG.


So, to override the variable that is set with ARG instruction during the Build step, we have to set the command as follows through command prompt

docker build -t <imnage_name> --build-arg "VARIABLE_NAME"= <overridden value>.


The same thing can also be done during the RUN time also. 

So, to do this we have to submit the command during RUN time as

docker run -it --name <name of the container> -e <variable_name>=<new value> <imnage_name>:version of the image.


Docker Volumes:


Generally we have a linux server which has our container. In container we will have some files.

If we loose the container for any reason, the files inside container (like jenkins jobs,property files), we will loose the data also.

In real time scenarios, we dont want to loose this data. We will keep it somewhere (outside the docker container area)  , and we will load that data in to a new container.

The new container, will contain all the previous data. For this purpose, we use something called "DOCKER Volume"

We can store the files both in docker daemon and host area . If we use Binmount in docker volumes it will be stored in host area (file system) .Other wise it 

gets stored in docker area /var/lib/docker.

So how do we decalre these volumes.

So, while running we give the command 

docker run -v <source path> :<destination path>

docker -v /data/my_data :/data/my_data ---> the path on the left hand side is host path and the path on the right hand side is the container path.

We can mentionn multiple -v s to keep multiple volumes. 

Please note that container data is ephemeral (meaning once the container is lost , the data is also lost).

To see the list of docker volumes created we need to give the command

docker volume ls. 

to create a new docker volume we need to run the command 

docker volume create --name <volume_name> 

But the volume_name folder wont be stored in ec2-homeuser directory instead it gets stored inside /var/lib/docker folder.
Within the docker folder we will have other folders named volumes and _data which is the exact location where the content related to our docker volume
is stored.

/var/lib/docker/volumes/mydata/_data.

So, if we discuss the problem.. suppose

we have to  run  a container named ubuntu inside the _data folder of the created docker volume.

Create  sample.txt file inside it and put some text in it.Then if we stop the container , the files inside it are lost permanently.

We cant retrieve that data unless we persistently store the data. To do this we have to make use of the following command

docker run -it --rm -v "volume_name":/<container_folder_name> --name "container_name" "image_name"

docker run -it --rm -v my_volume:/ (/ --> indicates the current container )my_data(folder within the container) --name "ubuntu_c" "ubuntu"

please note that if the folder within the container is not present it is created also with the above command.

So, once we run a container with the above command , it creates a new folder with that name and then we can keep 2 files inside it namely

sample.txt and some.txt. After that if we stop the container , and come out to the server workspace (/var/lib/docker), we will be able to see the 

2 files that were removed as part of closing the container above. This is because it has be stored in to the persistent storage of the docker volumes

Please note that only the files inside the folder that we mentioned in the docker run command above are stored into the docker volume area but not any other.

Here one more important point to note is, the data is copied from volume to container space and container space to volume in bi directional manner.

This means any changes to the volume area will automatically copy the contents into container and any changes in the container area will do the same 

in the reverse manner.

Exit command can be used we will lose the container if we start it with command docker run --it --name <name of container> <image of container>

however if we execute docker exec --it <docker id> and then  do exit, it wont remove the container permanently. it justs terminates the existing

terminal of that container.

So, far the data (i.e) the volume we created has been storing in the docker area. But, how to store it in the File system . For this we will use

bind mount

So,for this we create a dir in our ec2 user path (of our server), (we can even keep any txt file inside this path if needed)

then we run any docker image like below

docker run -it --rm -v <path_of the folder in our server which we created in the above step>:/<container folder where we would like to copy files from our host machine> --name <container name> <image name>

in this way the container can be persistently stored in the filesystem and its content get stored in the directory that we created.

the same above command can also be used by giving $(pwd) instead of absolute path (by replacing /home/ec2-user.


So, to summarize We can create volumes or keep a persisitent storage of contianers in below ways 

first create a docker volume
docker create volume <volume name>

docker run -it --rm -v volume_name:/container folder --name "container_n ame" "image_name"

or we can store inside the file system by directly running the command in bind mount by first creating a folder and then running container

docker run -it --rm -v $(pwd)/folder_name:/folder_name --name <container_name> <image_name>

Jenkins on Docker .

Lets take a scenario , where nexus,jfrog,docker private repo, jenkins having some data in container, and if the container goes down all
the data inside them goes down.But, Once we attach the data to a volume and if we run a container, we will get the data back.

Lets look at the example with Jenkins example.

please run a jenkins imaage using below command

docker run -d -p 8080:8080 -p 50000:50000 jenkins/jenkins:lts-jdk11 (the second -p with port number 50000:50000 represents maste-slave port service)

Once we run the above command jenkins is already installed and we can login to jenkins by using public ip and 8080 port.

But,as it will look for passwword which we can get from /var/jenkins_home/secret ---> this is setup in the docker file of the jenkins image that we 

downloaded.  With this single command, we can completely install jenkins .

After that, we can check by creating a job in the jenkins container and removing it. We can then check if the volume can keep the data persistently.

Generally, speaking all the docker images that we build are stored in dockerhub.But in real time there are other repositories like jfrog,nexus,ecr and docker private repo.

So, then i created a new job first job in the jenkins container and ran it.After that if i remove the container completely we will loose the data i.e the jobs which we created permanently

This is because we didnt keep the data in the persistent storage .

So, we have to keep the persistent volume to retrieve the data. This is done using the below command.

docker run -p 8080:8080 -p 50000:50000  -v $(pwd)/jenkins_home:/var/jenkins_home jenkins/jenkins:lts-jdk11 , but run it using a root account by giving


docker run -d -u root -p 8080:8080 -p 50000:50000  -v $(pwd)/jenkins_home:/var/jenkins_home --name myjenkins2 jenkins/jenkins:lts-jdk11

with this we will get our initial pass inside the folder jenkins_home of our file system. 

Then we can later create a new job and build it. Now, after this if we exit the container and remove it .But with the above command that we used

when we create a new container of jenkins, we will still be able to login to the new jenkins page with our old pass word and will be able to see the 

jobs that were created previously. This is due to the fact that we have used the persistent storage of the data in the above commands.


Also, please note that we have a command with option --restart=always, to ensure our jenkins service or any other service running on a docker
container may be automatically restarted after every instance reboot without the need of explicitly restarting it.

docker run -d -u root -p 8080:8080 -p 50000:50000 --restart=always  -v $(pwd)/jenkins_home:/var/jenkins_home --name myjenkins2 jenkins/jenkins:lts-jdk11


Docker private Repo:

Instead of using docker hub we are going to use docker hub private account ,our own private repo, nexus or jfrog or ECR.

all these can be usedto push our docker images.

To create our own private repo we need to install a software called "reigstry" this can be used to store our own images. This can be downloaded

from dockerhub. Go to docker hub search with registry and select the latest one.


so the command to start a registry container is as below.

docker run -d -p 5000:5000 --restart=always --name registry registry:2

Once the container is up and running we can push the image.

This is done by downloading any existing image like ubuntu and then creating a new ubuntu image (replica) of it and pushing it on to our repo
the following commands help us do these

docker pull ubuntu

docker tag ubuntu localhost:5000/ubuntu:v1

Similarly please do it for another image alpine.

Just repeat creating a new tag for v2 for both of the above images.You can notice that, while we do this operation the original image and the 

2 newly created images will be having the same ID.This means they all are acted upon at the same time and will work similarly at all times.

Now all we need to do is push these newly created images in to our private repo.

this is done by the command

docker push <image_name>:version number. Now, we can push all the 4 images using the same command by giving their specific image name and version.

As an experiment, now please remove the images by docker rmi command. Once we do this task, the images get removed.If at a later stage we want to
get the images back, we can do that by directly  pulling by below command

docker pull <image_name>:version_number. This helps the user to automatically download the image from the private repo which we created using registry container.

In this case please note that the images can be downloaded succesfully only when the container is up and running.Once that is deleted, the images cant be dowloaded again

So,to keep the images data inplace we have to put them in a persistent storage.This means we have to create a volume and put it there.

To run a docker container by keeping the volume option enabled we need to use the following command. This is picked up from the website 

https://docs.docker.com/registry/deploying/

docker run -d \
  -p 5000:5000 \
  --restart=always \
  --name registry \
  -v /mnt/registry:/var/lib/registry \ ---> here contents from /var/lib/registry are stored in to /mnt/registry 
  registry:2

with the above command executed, we can see that images that are initially pushed in to docker private repo using registry container can actually


be retained in volumes even when we try to pull them after the container has been removed. (But, first we need to push them in docker private repo)


docker run -d -p 5000:5000 --restart=always --name=registry -v /mnt/registry:/var/lib/registry registry:2

In docker private repo we dont have an UI. So,how to check all the versions available ?

We need to use Docker private registry rest api.


the command we use is curl -X GET http://localhost:5000/v2/_catalog

Storing Docker images in Nexus:

Just like Docker private repo we ca use nexus also to store the images .Along with nexus we also have Amazon ECR.

ECR is full form of elastic container registry. this is a service available in the AWS 

Once we login to our AWS account and select for ECR we get its details.  We create a ECR for ourselves.We then follow the steps to push the 

image in to the ecr .(We have to use AWS CLI for this).Also, we need to authenticate docker client token with the AWS ECR registry.

https://www.youtube.com/watch?v=O6wPavc-xuc



How to build docker images using jenkins :

We can do it in 2 ways

Install Jenkins and Docker manually and add jenkins to docker group

Second one is to create a dockerfile and finish it.

Lets see an example where we run jenkins on a docker instance by using the below command

We use the already exisiting jenkins image  and run a new container.
jenkins/jenkins lts-jdk11

docker run -d -u root -p 8080:8080 -p 5000:500 --name=jenkins_new -v $(pwd)/jenkins_home:/var/jenkins_home jenkins/jenkins:lts-jdk11 --->understand each and every part of this command correctly.

Then we can open the jenkins page on 8080 port in our server.Later create a docker test job and run it by giving a sample docker command like

docker images. If we run our job, we will see that our job fails.


To solve this problem we need to add the volume of our docker. for this the command is 

docker run -d --restart=always -u root -p 8080:8080 -p 50000:50000 -v /Jenkins_home:/var/jenkins_home -v $(which docker):/usr/bin/docker -v /var/run/docker.sock:/var/run/docker.sock --name jenkins5  jenkins/jenkins:lts


-v /Jenkins_home:/var/jenkins_home -v $(which docker):/usr/bin/docker -v /var/run/docker.sock:/var/run/docker.sock ----> please understand all the volume creations in this command clearly.

all these reperesent in  serial manner from left to right ---> container data, update the softwares that are installed in our container , run the docker daemon (so that we dont have to restart it again) and 

After wards we have a new jenkins page and then we can login to that jenkins page by using the credentials that are given on the UI page of Jenkins

/var/jenkins_home/secrets/initialAdminPassword

After this we can then create a new jenkins job and run a sample command like docker version which in turn will provide us the docker version number.

But,the above is  a very very basic job.Lets take a look at a free style job with jenkins. In this task, we will also make use of a basic Dockerfile

that will run an application based out on Java

The freestyle job with jenkins contains the following steps to be completed.

git (code pushing) -->jenkins (fetch the changes|)--> build (build the docker image) -->push the built image to Docker hub or ECR.

The dockerfile we wrote looks like the below:

FROM openjdk:8
COPY HelloWorld.java HelloWorld.java
RUN javac HelloWorld.java
CMD Helloworld

After writing this docker file we add ,commit and push this file on to our GIT hub repo.

Later return to the Jenkins page and add our git hub credentials and trigger build.

Now, to push this built image on to docker hub we go back to our job page and check if we have any option to do that.

Unfortuantely, we dont find any option to do itl Hence, we have to download a plugin named

cloud bees docker build and publish.

Download this plugin and complete the required crdentials Then if we trigger the build it will automatiacally push our image to our dockerhub,

Now,check how we can push a docker image in to docker hub using jenkins pipeline method rather that free style.

Jenkins pipeline code plugin is allowed by default in Jenkins.

so the pipeline is generally written as

pipeline{
agent any{
stages{
	stage(){
		steps{
			}
		steps{
			}
			
devsecops --> at everystep there will be security check .this is called devsecops.

The user has to create a pipeline job and then choose a basic java project (to begin with just a hello world project is fine).

Then, we have to write a pipeline script. 

The pipeline script may follow the below structure

set environment -->clone  the build --->build the image --->push the image  ---> remove the image.

For this to happen succesfully, we need to install docker pipeline plugin.

Now let us see the part of pushing our image in to ECR .

First we start our Docker instance. Then we need to create a new repository inside the ECR.

We can create a new repository by name tomcat_app in ECR.

ECR can be logged in using the AWS CLI or directly from the UI console.

For integratiing ECR with jenkins we have to install a plugin called Amazon ECR . Then we need to understand that have to givee

permission for jenkins job to access AWS service. This is done creating a user called Jenkins in AWS IAM service and providing it 

with full admin rights and permissions. After that to access the AWS console by the jenkins job we have to get full access key and secret key.

These are generated within the AWS console and kept aside.

Then we have to write the pipleine code as below.

The pipeline code follows in this manner

pipeline declaration --> tools  being used(maven in this case) ---> environment setup (version number of image project name,image number,ecr url,ecr cred),

stages in the pipeline like build war file, building image and pushing image (for building and pushing image we use docker.build and docker.pushwithRegistry commands embedded
inside a script) and finally we remove the image that is pushed in to ecr registry everytime as it will be unnecessary data in the ecr registry.


pipeline{
	agent any
	tools{
		maven "maven 3"
		}
	environment{
	VERSION="${BUILD_NUMBER}"
	IMAGE="tomcat_app"
	PROJECT="$IMAGE:$VERSION"
	ECRURL = "133538478978.dkr.ecr.us-west-1.amazonaws.com/tomcat_app"
	ECRCRED = "ecr.us-west-1:aws_jenkins_credential"
	}
	
stages{
	stage('get scm'){
		steps{
			git credentialsId:'docker_jenkins_github',url:'https://github.com/shashanksrinivasa9/spring3-mvc-maven-xml-hello-world'
			}
		}
	stage('build'{
		steps('build war file'){
			{
				sh "maven package"
			}
		}
	  }
	stage('build image'){
		steps{
			script{
				docker.build('$IMAGE')
				}
			}
		}
	stage('push image'){
		script{
			docker.withRegistry(ECRURL,ECRCRED){
			{
			docker.image(IMAGE).push()
			}
		}
	  }
    }
  }
	post
	{
		always
		{
			sh " docker rmi $IMAGE | true"
		}
	}
 }

Similarly to setup a freestyle job that performs pushing docker image to ECR .

we have to create a new free style job. Put the required git hub url and credentials

give the details of maven maven version and goal)

Use docker build and publish section and enter the details as required.


Multi configuration Jobs:

In real time three environments will be present development,pre-production and production. Then if we want to deploy a single job

on to multiple environments at one shot. Then we use of something called multi config jobs.

In jenkins home page, we have multi configuration job section.

Then we need to take a git repo where we will be having the files related to all the 3 environments in a single folder.

Docker compose:

Whats the use case of Docker compose:

If we have 10 images in our project how do we run 10 docker images. Whats the command Docker run.

If we 10 containers, we have to run the command 10 times.SSimilarly we have to give docker stop 10 times also to stop them all.

So, for this purpose we use docker compose file.

Every docker compose has a version .Docker compose and docker software version should match at everytime.\

So,first install docker compose from the link : curl -SL https://github.com/docker/compose/releases/download/v2.20.2/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose

This gets installed in /usr/local/bin/docker-compose.

Please note that docker-compose is fianlly an executable file and we can give it executable option by the command

chmod +x /usr/local/bin/docker-compose

We can check the version of docker compose by typing the command docker-compose --version

Docker compose is a file with collection of all docker commands and to convert them in to a .yaml file.

The docker compose file starts with the command Services --> divides the number of container and represents  the individual  container name, it also carries information related to ports,volumes

of that particular container.

So, to implement a project using dockercompose model  we need a dockerfile, dockercompose file and we need to run the dockercompose up command to start the  containers.

We have to first install docker compose using the docker installation command from the web server.

docker compose  gets installed on /usr/local/bin/dockercompose ---> we need to give permissions by giving chmod +x 

We can make use a flask framework (based on python ) project to check the real time example of a docker compose exanmple.

Create a folder in our ec2 instance and name it docker-compose. Then, we can copy a sample python file named App.py.

This App.py , does the task of checking how many times a webpage is hit and stores its value inside a data base called "REDIS"

"REDIS" runs on the port 6379

App.py code 


import time
import redis
from flask import Flask
app = Flask(__name__)
cache = redis.Redis(host='redis', port=6379)
def get_hit_count():
 retries = 5
 while True:
 try:
 return cache.incr('hits')
 except redis.exceptions.ConnectionError as exc:
 if retries == 0:
 raise exc
 retries -= 1
 time.sleep(0.5)
@app.route('/')
def hello():
 count = get_hit_count()
 return 'Hello World! I have been seen {} times.\n'.format(count)
if __name__ == "__main__":
 app.run(host="0.0.0.0", debug=True)


We will need to install both the dependencies of the python file (redis and flask) .

So, we create a separate requirements.txt file and copy both the files inside the requirements.txt file and provide them as inputs to the PIP installer.

For this purpose, along with the Flask we can also install redis in a separate requirements.txt file.

We should then write a dockerfile like below

FROM Python:<version_number>-Alpine  
COPY . /code
WORK DIR /code
RUN pip install -r requirements.txt
CMD['python','app.py']


The steps in the above docker compose file can be understood as below.

FROM <> -<> --> download (build) a particular image and its version.

COPY ---> copy all the files from the current path unto /code folder of the container.

WORKDIR ---> move the working directory to /code

RUN ---> install the softwares from  requirements.txt using the pip installer during the image building time

CMD ---> runs the python app after container build command is given.

We can then write our docker-compose.yaml file as below.

version: '3'
services:
 web:
 build: .
 ports:
 - "5000:5000"
 redis:
 image: "redis:alpine"
 
 This is representing the version of docker compose file, "services" ---> represents the division from where the containers can be created.
 
 web/redis are the container names. build builds that particular image version (like web latest version) and if we want a specific version to be installed
 
 we can mention it using the "image" tag. "Ports" is used to expose the port number on which that particular container's service is running.
 
 we then make the docker-compose up by running the command docker-compose up .
 
 Check out awesome docker compose github repo to see more examples on docker-compose.
 
Docker Networking:
 
How are 2 containers communicating. How does an ip address get assgined to a container. how to create a custom network.

Lets see all details about docker networking.

In general once docker is installed a default network named docker0 (bridge) is automatically created. There are 2 other networks created along with this 

named host and none. This can be seen using the docker command docker network ls.

Whenever a vm or a server is created on top of machine the internet capability on the created server or VM is enabled by the help of the bridge network.

Also, as soon as we run a container we can see a new network named "veth" getting created. This represents the network of the  newly created docker container.

Please note that for all the containers that get created, the default bridge network i.e named as docker0 gets mapped to newly created container networks (like vetho ,veth1).

To understand what will be ther ip addresses that get assigned to our containers we can make use of the command

docker inspect container <container_id>.

Now, to check if 2 containers running on the same default bridge (docker 0) are communicating or not, we need to install networking tools using the command.


apt-get update && apt install -y iputils-ping && apt install iproute2 -y. Please note that we have to install these tools on both the instances.

Then we can ping the second container from the first container bash using ping <ip_address> commnand.

Now, to check a scenario, where we have a custiom built network, with 2 containers trying to connect to containers inside the default bridge network.

For this to happen we first need to create a custom built network on top of that we have to run 2 containers (install networking tools on them).

docker network create mynetwork --subnet=10.0.0.1/16 --gateway=10.0.10.100 ---> this will create a custom network with 2 power 16 CIDR block

After, creating a nee wnetwork and containers inside that , we can try and conncect the container inside our new network with the default network containers using the below command.

docker network connect <bridge> <new_container_name> After this we can ping the containers in default network from the containers in the new network.

Ansible:

What is a configuration management.

To execute a command on multiple servers at a single point of time we use Configuration management tools

Some examples of configuration management tools are 

1)puppet
2)Chef
3)Ansible
4)Salt stack

All the above do the same job, there will be differences in the process in which the task is done

Among all the configuration tools from above, Ansible is the most popular one.

Lets say there are 4 servers and we have to install Javaon all 4 of them. The general process is to SSH each server and install the software.

Generally, each server has its own IP address.

Now instead of manually updating the installation on each server separately, we can write a script "install java" in .YAML file. Similarly we will write

the script to uninstall. Inside the .YAML installation and uninstallation files, we will give the ip addresses on which whatever task has to be performed.

We can continue to add the requirements by giving the ip addresses. So, the same command or instruction applies on all the IP adderesses at a single go.

So, the benefit of this type of management is there is no need to login to each server and do the installation manually which also reduces the human error.

This is also called as "Infrastructre as a CODE".

The main difference between Ansible and Chef is as below.

Ansible is agentless configuration management

Chef is Agent based.

In Ansible we dont have to run any agent or softwares on nodes. But in Chef, we have to install and make sure chef client is running in every node.

Ansible is push based mechanism. This means there is no need to install agent and server itself updates the nodes whenever there are changes.

Whereas Chef is Pull based. This means Agent checks server for any changes to be implemented.

Ansible carries an inbuilt SSH whereas Chef doesnt have an inbuilt SSH.

Ansible is implemented using playbooks in YAML format. Chef is implemented using Cookbooks.

Ansible is written in Python and Chef is written in Ruby.

Ansible terminology:

Ansible host --> the machine where ansible is installed and ansible playbook is running

Ansible nodes --> all the other machines on which ansible performs operations.

Inventory file --> the file where IP addresses are written. It is also called as host file.

Scripts that we write are called playbooks.

Inside polaybook we will have Tasks, Modules, Roles.

Ansible Galaxy --> Aansible playbook are stored in Ansible galaxy.

Ansible configuration file --> all the details of the ansible setup 

To understand ansible in a better way we need 3 hosts. First is the ansible host and the other 2 are nodes/slaves.(make sure it is ami2)

Then we install ansible using the command sudo yum amazon-linux-extras install ansible2.

ansible --version gives the version installed ..it gets installed in /etc/ansible/ansible.cfg

we can see that initially, ansible hosts file remains filled with some default data. Generally, as discussed above it should be containing the IP addresses

of the machines on which we will be performing the actions. We can completely remnove all the data from the hosts file.

ansible.conf file contains lot of default info with information related to settings.

To begin the ansible implementation open the hosts file and edit its contents.We can create 2 sections like [server] [node] and keep server private ip

and nodes private ip addresses in it.

Now to check whether the server and nodes are able to get connected, we need use the "ping" module

We use the ping module by tyoing the command ansible -m ping node, ansible -m ping server ansible -m ping all

But we will see the problem of permission denied when we do this as ssh client communication wont be established without the .pem key

So to establish the connection, we can a create a new .pem key folder in vi editor in the same path, copy the contents of the .pem key from the server

that is present in our windows local machine and paste it in our new txt file.Later on, we can try pinging again by using the following command.

(Make sure you create the .pem file using the sudo permissions).

ansible -m ping -u ec2-user --privatekey <key> server/node/all .The key here is the one which we used to connect to our server.

But, with the above type of instructions, we will see a python interpreter warning. This happens even after a succesful pong in repsonse to our

ping to the server from the ec2 machine. The same information can be found from the website https://docs.ansible.com/ansible/2.9/reference_appendices/interpreter_discovery.html

Here we will see that the inbterpreter name expected is ansible_python_interpretern and the value it needs is the python path where it got installed in our

machine.This can also be be found using the command whereis python.

Set the python interpreter name and value inside the hosts (inventory file) for all the three machines (i.e one server and 2 nodes). has to be given.

If we set the value in this way and run the instruction again, we wont be seeing the python interpreter warning message.

But, if we check carefully, the command ansible -m ping -u ec2-user --private-key=<keyname> server is actually exposing username and key in the command directly.

To avoid this we can set these values directly inside the hosts file using the parameter 

ansible_user =ec2-user, ansible_ssh_private_key_file=<key_name> after we do this we can directly use the command ansible -m ping all and it directly pings all the machines succesfully.

But, with this kind of approach, the hosts file gets really messy .So , we can create a new section called [all:vars] and put all the parameters with their 

respective values on top of the file .Then we can remove the repetition of these instruction within the file to ensure the file is not messy.

Ansible Adhoc commands:

Adhoc commands dont get stored in any file. They are simply used for onetime execution
Adhoc commands work on many modules.

We can check the total modules in ansible using ansible module documentation.

Some examples are :

ping

command

shell

service

copy

file

as we have already seen ansible -m ping all ---> will ping all the machines.

ansible -m ping server/node/group name --> will ping the particular group

ansible -m ping ipadress --> will ping the particular ip

ansible -m ping 172.128.x.x ** --> will ping the nodes with that particular ip addresses.

ansible -m ping !:<ip_address> all ---> all ip addresses in the group are executed except the one beside ! mark.


Copy module:

Basically if we want to copy some files from one machine to another we use copy module.

Every module containes some parameters .These are options that are needed to work with modules.

For example, in copy we will use source and destination as parameters.

we can check an example of copy by creating a .txt file in our server like shashank.txt and copying it on to all the machines in the node group using below

statements

ansible -m copy -a src='/etc/ansible/shashank.txt dest=/home/ec2-user/shashank.txt' node -- this copies the file shashank.txt on all the ips in the node group

command module:

if we have 100 servers and we want to see disk space or ram info we can use the module command.

ansible -m command -a 'df -kh'all --> this will give the info related to disk free for all the machines

For example if we want to check 2 commands together like diskfree and ram info like df -kh and free .How do we do it with

ansible? we cant use && as in the case of linux using command module. 

If we want to work with multiple opeartors we use shell module.

ansible -m shell -a 'df -kh && free -kh' all

File module:

If we want create a file/directory/change its permissions we use the file module.

In ansible, if we want to create or delete a file or folder we use the file module with "state" argument carrying multiple values to it like absent,directory,file,touch,hard,link

For example, to create a file in our path we use file module along with arguments dest and state. dest is used to create the file in the particular loaction

and state is used describe the action to be performed on that particular file.In this case, it will be 'touch'

so, the command will look like

ansible -m file -a 'dest=<name_of_fle> (gets created in the /home/ec2-user path)  state=touch'

within the file module we can also use a parameter called "mode" to change the permissions of  a file.

ansible -m file -a 'dest=<file_name> mode=<value of the permission>' node

if we want to change the permissions of the file while creating it we can use the following command

ansible -m file -a 'dest=<file_name> state= touch mode=700" node --> this will both create a new file and edit its permission also 

ansible -m file -a 'dest=<name_of_fle> (gets created in the /home/ec2-user path) state=directory' this will create a new directory.


ansible -m file -a 'dest=<name_of_fle> state=absent ' this will delete the directoty or the file 


lineinfile module:

we use lineinfile module to add data in to an existing file
ansible -m lineinfile -a 'dest=<file_name> line="hi how are you"' node


yum module:

if we are using ami linux we can use "yum" module to install software

the yum module uses argument called "state" which takes values as install,present, absent  which represnt installing (first 2 and uninstalling respectively)

ansible -b -m yum -a "name=httpd state=present" node (-b represents sudo user).

service module:

Even though we install a software using yum module to make sure the service is up and running we use service module.

The service module takes the argument "state" with value restarted reoloaded,started stopped so to start we use started option

ansible -b -m service -a "name=httpd service=started" node


In real world instead of adhoc commands we generally write playbook.

Playbook contains set of tasks and modules

For example we want to install httpd, or create a file these are all called tasks. The same playbook can be used multiple times

Playbook syntax is generally written in YAMl format.

Playbook syntax is below (an example of creating a file in ansible playbook)

--- --> it starts with three -
- hosts: node --> indicates where does the playbook implemenmt
  tasks: --> indicates the task to be performed (part of the list of tasks )
  - name: create a file  --> (name of the task)
    file:  --> module name. even though it contains : it is not exactly a map in ansible context. just represents an ansible module
      dest=sample.txt ---> name of the file to be created
	  state=touch   --> state field is used to create the file

Connect the three servers and write the above yaml file in the /etc/ansible path of the  master server .

Then we can run the playbook using the below command.

ansible-playbook.<palybook.yml_file> is the command to run the playbook
before running the playbook we can also check its syntax by using the below command

ansible-playbook <playbook.yaml> file --syntaxcheck



similarly we can write multiple plays (tasks) inside a playbook using nultiple tasks field

---
- hosts: node
   tasks:
   - name: create a file
	 file:
		dest=shashank.txt
		state=touch
   - name: create a directory
	 file:
	   dest=devops
	   state=directory
   - name: change mode
	  mode:
	  dest=shashank.txt
	  mode: 0400 (pleasenote that in ansible mode module has to be set in "octal mode" only)

Now, to write content inside file we need to use the copy module and it contains parameter called content to which we can set up a | pipe symbol

so the yaml file looks like

---

- hosts:node
  tasks:
  - name:create a file
    file:
	  dest=file.txt
	  state=touch
  - name create a directory
    file:
	dest=my_dir
	state=directory
  -name: copy content
   copy:
    dest=file1.txt
	content= |
	   <content>
	   <content>
	   <content>

Now, the next scenario is to create multiple files 


---
- hosts:node
  tasks:
  - name:create a file
    file:
	  dest=file.txt
	  state=touch
 - name:create a file
    file:
	  dest=file1.txt
	  state=touch
 - name:create a file
    file:
	  dest=file2.txt
	  state=touch
 - name:create a file
    file:
	  dest=file3.txt
	  state=touch
	  
The above yaml file works perfectly with out any issues. But, we have created duplication of items. 

So, to remove this problem we use loops  using item and with_items keywords. the keyword path contains the value item inside a flower bracket

where as with_items is a list , showing all the required files that have to created in a looped manner.

The path keyword just checks if the file required is present in the path (by substituting it with item) and then inside the with_items section

files are evaluated one by one.All the other actions are also executed one by one.
to see thesyntax:

---
- hosts: node
  tasks:
  - name:create a file
    file:
      path: "{{item}}"
	  state:touch
	  mode: 400
	with_items:
	  file1.txt
	  file2.txt
	  file3.txt
	  file4.txt
------

So, how to install git software on all the nodes.

If we dont have a configuration management we have to install git individually on each seerver.

Using configuration management , we can install using a single command. Generally playbook looks like below

---
- hosts: node
  become: true
  tasks:
  - name: install git 
    yum:
	 name: git
	 state: present

With the single above playbook we will be to install git on all the available nodes.

Please note that we can also mix and match the type of amazon machines while executing an ansible playbook.

But, for this to happen, we haev to update the inventory (hosts) file with the ubuntu environment variables in the [ubuntu:vars] section.

this includes user as ubuntu , key of the ubuntu ec2 machine ,path of python  and also add unode IP address.

We also need to write a separate ansible-playbook for ubuntu machine .Please remember that the module name that ubuntu uses in APT but not YUM.

now to check a scenario where we have to uninstall git from the above playbook

---
- hosts: node
  become: true
  tasks:
  - name: install git 
    yum:
	 name: git
	 state: present
  - name: uninstall git
    yum:
	 name: git
	 state: absent

when we run the above playbook it will perform botht the tasks. In case if we already had git in our machine,it will try to install it again.

But, in this case ansible provides us an option wehere we can run only the required tasks and skip the others depending on our choice.

This is done using the concept of tags . We can write a separate section in ansible playbook using the keyword ansible like below.

Please note the every task can be mapped to a specific tag and we can mention whether to install or uninstall


---
- hosts: node
  become: true
  tasks:
  - name: install git 
    yum:
	 name: git
	 state: present
    tags:
	 install
  - name: uninstall git
    yum:
	 name: git
	 state: absent
	tags:
	 uninstall

this can then be implemented while running the ansible playbook using the command

ansible-playbook <play_book.yaml> --tags <tag_name>. With this, we can only perform the tasks (internally module)  that are tagged to a particular tag.

Install Maven :

To install maven , we can use ther below playbook and install it.
---
- hosts: node
  become: true
  tasks:
  - name: install maven
    yum:
	 name: maven
	 state: present

Ansible Galaxy:

Just like git, docker have githubs and docker hub ansible has ansible galaxy.

Ansible galaxy we can see system level playbook, development playbook and all other varieties of playbooks.

We have to see Ansible role details. Through ansible role we wrtie ansible playbook in real time.

In real time, when we have multiple servers with various requirements. Like installing git,maven,java in one then just git on another after that we have 

machine where we need to install git maven java and then uninstall one of them. Clearly with whatever we have learned so far,we have to wrtie multiple palybooks

where repeated actions would be performed.

Instead of doing it  we will first create roles like git,maven,nginx etc. Then in the playbook that we write we just mention the role name in the place of the

tasks that we wish to perform. In this way, only that particulr role will be implemented. Additionally, we will write separate playbooks for each of the required

software and we will keep the needed steps inside them. So, when we actually run the playbook having our role name in it we will implement the playbooks of the softwares

that are essential to be installed.

So, example playbook with roles of git,maven,java looks like below

----
- hosts: node
  roles:
   -git
   -maven
   -java
   
So, when we create a role it internally contains many folders like tasks,variables,files,handlers,templates each containing all the important data

or dependencies that are required for an ansible role to work correctly.

So, a "ROLE" can be defined as a complete unit  of automation that can be reused and shared.


So, when role is created a default folder structre is created.

files

templates

vars
  (contains main.yaml)
meta
  contains main.yaml)
handlers
  (contains main.yaml)
tasks
  (contains main.yaml)
default
  (contains main.yaml)

So, how to create a role?

To create a role the command used is ansible-galxy init <role_name> , so for git it will be ansible-galaxy init git.

From the above discussion, we can confirm that main.yaml file that is inside the tasks folder is the folder where our code first starts to work on after we create role.

so, to summarize the code we need to do as below.

create a root in /etc/ansible folder using the command ansible-galaxy init git

inside the hosts/inventory file.


update the ip address of the ubuntu instance (create one if not exisitng) and update its dependecies

in the main.yaml file of the tasks folder, update the code as below

---
 apt:
  name=instll git
  state=present

along with this,in the main root folder (i.e./ansible/etc) ,create a new yaml file and update as below

---
- hosts: unode
  become: true
   roles:
    git
	
In the above process we run the yaml file which is present in the /etc/anisble. As noted above it just contains the folder with the role name.

So, that role folder inside /tasks folder is called which contains main.yaml file, in which we have already written the tasks (i.e the script needed to

install the required software ,in this case GIT).

Also, in the inventory file,dont forget to update the details of the new ec2 server.

The key point to note here is that, in main.yml file inside the tasks folder of git role folder, we only update the module and its name and state.

The other yaml file which we create outside in the ansible root folder we mention hosts: unode, give root permissions and also metntion the role name using roles option.


Apache tomcat in ansible:

Pre requisities to install apach tomcat are:

install jdk
tar tomcat
untar tomcat
modify tomcat-users.xml
modify context.xml
copy the war file in to web apps
start the tomcat service

So, we need to install all of the above first to start the process of tomcat installation using ansible.

So, first create a role called TOMCAT and go in to TASKS folder which contains main.yaml file.

inside the main.yaml file, we can start writing as

---
- name: install tomcat
  command: wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.65/bin/apache-tomcat-9.0.65.tar.gz
  args:
   chdir: /tmp/
- name: install jdk
  yum:
   name: "{{item}}"
   state: present
   with_items:
  - java-1.8.0-openjdk
- name: Extract tomcat version
  command : tar zxvf /tmp/apache-tomcat-9.0.65.tar.gz -C /opt/ 
  creates: /opt/apache-tomcat-9.0.65
- name: copy war in to web app
  copy:
   src: /etc/ansible.sample.war
   dest: /opt/apache-tomcat-9.0.65/webapps/sample.war
- name: start tomcat service
  command: nohup /opt/apache-tomcat-9.0.65/bin/startup.sh &
  
After we finish writing the above code in main.yaml file of the tasks folder, we should write another yaml file in the tasks folder section

this should contain the followinjg information

---
- hosts: node
  become: true
  roles:
   tomcat

After the playbook runs successfully tomcat is installed on the nodes.We have to change 2 files i.e tomcat-users.xml and context.xml located

in conf and web app folders respectively yo make sure we give the tomcat credentials for logging in.

content to be written in tomcat-users.xml  -->please note this content only has to be added but not edited or changed in any way.
<role rolename="admin"/> 
<role rolename="admin-gui"/> 
<role rolename="admin-script"/> 
<role rolename="manager"/> 
<role rolename="manager-gui"/> 
<role rolename="manager-script"/> 
<role rolename="manager-jmx"/> 
<role rolename="manager-status"/> 
<user name="admin" password="admin" roles="admin,manager,admin-gui,adminscript,manager-gui,manager-script,manager-jmx,manager-status" />

In this way we can install tomcat on to different nodes using ansible.

Install Nginx with custom page:

Connect the instances and create an index.html page.

we can enter the following content in it
<html>
<h1> this is my page</h1>
</html>

We have to enable the 80 port in the inbound rules of ourt security group for all the nodes as nginx runs on 80 port.

then please create a install_nginx.yml file like below

---
- name: install nginx
  hosts: node
  become: true
  tasks:
    - name: installing nginx
      command: amazon-linux-extras install nginx -y
      args:
        creates: /sbin/nginx
    - name: copy the index.html file
      template:
        src: index.html
        dest: /usr/share/nginx/html/index.html
    - name: start nginx service
      service:
        name: nginx
        state: started

 Please note that we have command,template and service modules in the above code. Command keyword to implement the command to install nginx,
 
 template module to copy our customized index.html in nginx instead of the default one and finally service module to start the nginx service.
 
 With this we can see our webpoage loading on our EC2 instances.
 
 But, consider a scenario where we have made a change to our index.html file. If we re run the playbook we will observe that restart  step  (restarting the service)
 is again executed (in fact all tasks get executed again). This is fine and expected. But the same behaviour is observed even in the case when there are no apparent changes to the code.
 
 So, to avoid this situation of redoing tasks ,ansible uses concept called "handlers". Handlers are tasks only run when notified .Each handler has a global 
 unique name.
 
 so, the ansible playbook looks like below.
 
 
 ---
- name: install nginx
  hosts: node
  become: true
  tasks:
    - name: installing nginx
      command: amazon-linux-extras install nginx -y
      args:
        creates: /sbin/nginx
    - name: copy the index.html file
      template:
        src: index.html
        dest: /usr/share/nginx/html/index.html
	  notify:
	   - Restart nginx
	  ignore error: yes
    - name: start nginx service
      service:
        name: nginx
        state: started
   handlers:
    - name:Restart nginx
	  service:
	   name: nginx
	   state: restarted
 
 So, the trick lies with the keyword notify where we mention the handler name . Handler internally works like a task again.
 
 So, handler task is written separately inside which we mention its name (the same name which we will give in notify section) use service module again  and give the name and also status as restarted.
 
 We can also use the option ignore errors: yes to skip the execution getting stopped at tasks that cause errors. In this way, we can continue to see the code 
 
 getting executed with out breaking .
 
 To protect our playbook we use ansible vault. With this we can  protect our sensitive data.
 
 If we have to create an file ansible vault the command is
 
 ansible-vault create <file_name.txt> it asks for pwd, which we can  set.Once this is set every next time anyone tries to open this file.
 
 it asks for the password without which it cant be openend. To view the file that has been created with ansible vault the command needed is
 
 ansile-vault view <filename.txt> and to edit it the command is ansible-vault edit <filename.txt>
 
 if the file is already existing and then we want to protect it we need to use the command ansible-vault encrypt <filename.txt>, It asks for pwd,
 
 we can set pwd and encrypt.
 
 ansible-vault decrypt filename.txt will decrypt the file and can be viewed normnally
 
 if we want to change the password we can use the command ansible-vault rekey filename.txt and the  setup a new password.
 
 to run a playbook that is encrypted using the ansible vault we have to run the command ansible-playbook --ask-vault <playbook_name>.yml
 
 which will ask us to enter the password. Once we enter the required password, we will be able to run the playbook.
 
 Passwordless authentication:
 
 To setup a passwordless authentication (connecting a user from one server to another machine)
 
 Once we login to a machine we create a user. Now if we want this user to get connected to another machine that too without password 
 
 we create a private and public keys using the ssh-keygen command.
 
From the above generated public key, we copy the content and then we login to the second machine using the same username.

Once we login to the second mnachinen we go Inside the .ssh folder and create authorize_keys folder where we will be copying the public key of the first machine.

Once we do this we can then login to second machine from the first one using the command ssh@<username> <ip_address of second machine> with out the need of privater key.

Also if the server has a password we can connect to the server using the command ansible -m ping --ask-pass <node> (--ask-pass command prompts us to give the required password).


Monitoring Tools:

Once we run an application , to check its status we need to check log files.

If we have one machine , we will directly go the machine and check logs.

But, if we have 100s of machine with different applications we can monitor all the important things like cpu memory,full memory, application down

we have a centralized dashboard from where we can check all the issues together.

The tools that are used in market for this purpose are:
zabbix
nagios
grafana
prometheus
elk (elasticsearch logstash kibana)
datadog
appdynamics
splunk
dynatrace
aws cloud watch

We can check all the devops tools together in a devops periodic table @  https://digital.ai/learn/devops-periodic-table/

How does the monitoring tools work:

Suppose we have a zabbix server and linux server.Linux server sends logs to zabbix server and then we send a query like we do in SQL to check 

a specify data is shown and then we create a dashboard and we can see all the details in that dashboard.

To get this done, we need to install "agents" in the linux servers. Also we have to configure the server details (ip address ) has to be given.

Now, once the logs are copied from the linux servers to the monitoring tools server in the monitoring tools server we can query the data and we can setup

dashboard for the same. Also, if the data value exceeds the threshold we can also receive the notification alert.

But in prometheus and grafana we will have pro/grafana servers and linux servers with agents too.

In these 2 monitoring systems, the process follows a pull based mechanism. Where we will have the details of the agents configured in the prometheus server

So, the prometheus server works on pull based mechanism instead of push based mechanism.

Prometheus basically can be termed as a data model (time based) where data is identified by using metrics and keyvalue pairs.
Promql is the language used for this functionality.


To setup a prometheus monitoring machine set up an ec2 instance in aws. Then we connect the EC2 instance and download the prometheus software

using the command curl -OL https://github.com/prometheus/prometheus/releases/download/v2.47.0-rc.0/prometheus-2.47.0-rc.0.linux-amd64.tar.gz

we can then extract it using the command tar -xvzf prometheus-2.47.0-rc.0.linux-amd64.tar.gz

Once we extract the zipped folder we need to go in to the prometheus folder prometheus-2.47.0-rc.0.linux-amd64  and and we will observe a .yaml file 

named prometheus.yml

What we observe is as below.

global:
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          # - alertmanager:9093

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: "prometheus"

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ["localhost:9090"]

The sections of the above file can be described as below:

alerting:

alertmanagers: This section specifies the Alertmanager instances that Prometheus will communicate with. In your configuration, there is one Alertmanager instance defined.
static_configs: This is a configuration section that specifies static targets for Alertmanager instances. The targets field inside static_configs should contain the address (hostname and port) of the Alertmanager instance. However, in your example, the targets field is commented out (indicated by the # symbol), so it's not specifying any specific targets. You should uncomment this section and provide the actual address of your Alertmanager instance.
rule_files:

This section allows you to specify rule files that contain alerting rules for Alertmanager to evaluate. Alerting rules define conditions for generating alerts based on Prometheus metrics.
In your configuration, two rule files are listed but are also commented out (indicated by the # symbol). You can uncomment these lines and specify the actual paths to your alerting rule files. These rule files contain the conditions and actions to be taken when specific conditions are met.
scrape_configs: This is a top-level section in the Prometheus configuration file where you define the scrape configurations for different jobs.

- job_name: "prometheus": This is a job configuration section for a job named "prometheus." Each job represents a set of targets that share the same configuration.

static_configs: This section specifies the list of static targets that Prometheus should scrape for metrics data.

- targets: ["localhost:9090"]: Within the static_configs section, you define the targets by specifying their addresses. In this case, you have one target specified, which is "localhost:9090". This means that Prometheus will scrape metrics from a Prometheus instance running on the local machine at port 9090.
 
 
But, in the very first attempt we dont change any of the content from above and we copy and paste the prometheus IP address with its service port number.Remember that prometheus runs on port 9090. 

Similarly if we do http://13.57.229.83:9090/metrics we get the data of all the metrics (end points) the prometheus is configured for us.

Also in the main prometheus page, we can see the specific metric's data by enetering its name in search bar and clicking exectue .We are basically

executing promql commands to get the output in both tabular form and graphical representation.

Node exporter:

What is an exporter. 

To monitor anything on a server by prometheus some one has to send that data to prom. This is done by agent , it is called as "Exporter"

This exporters send details about the data in the servers.Example of this include linux instance metric (CPU, RAM,Network i/p and O/p, Processes running etc).

So, all linux machines can be monitored by using node exporter (jenkins, tomcat,nginx). Node exporters are of many types like Database,Hardware related, http ,api kind of exporters.

To check the working of node exporter install node exporter in our prom sever. Please note that node exporter runs on 9100 port.

Our prom server can also be monitored by agents or node exporters . To do this, after installing node exporter on our machine, we can go to 

prometheus.yml file and in  scrape config section we keep the target as localhost:9100 by adding a new job under static config section. 

Remember that each static config section under scrape config section represents a new job. Inside this sections we give our target IP addresses.

This means we are informing the prom to monitor itself on 9100 server by adding a new job.
 
9100 port.

Install a noden exporter using the wget command and wget https://github.com/prometheus/node_exporter/releases/download/v1.6.1/node_exporter-1.6.1.linux-amd64.tar.gz

once it is downloaded we need to extract it using tar -xvzf node_exporter-1.6.1.linux-amd64.tar.gz

This will download nodeexporter along with supporting files\

We need to go to the node exporter folder and run it using ./node_exporter executable. We can see that node exporter runs on 9100 port.

Inside this port we can see metrics link also.

But in our main prometheus service page inside the targets link only one node is running.Which is not the node exporteer but just a plain prometheus server data metrics.

Now to see the data metrics of the node exporter also,we need to add the data(ip address) related to that particular node in the prometheus.yml file.

Basically, we need to add a new job related to this particular node in the config section.

That will basically look like below.

- job_name: "node_exporter"

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ["localhost:9100"]

Once we make this change in prometheus.yml file we will be able to see node metrics also in the prometheus page.

We can then see the data metrics usingn the queries present in the prometheus UI.

There are also multiple prometheus functions. like rate(),irate() etc.

Now, what we have observed so far is we have montired the same prometheus server normally and through node exporter 

But what to do if we want to monitor a machine that is not our prom server?

For this purpose also, we use a separate server, download the node exporter. After that, we return back to our prometheus server

and then in the prometheus.yml file inside the scrape_config and static_config section we add then new machine's IP address in a sepearte job.

the configuration would like below for this.

- job_name: "server"

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ["18.144.40.216:9100"]
	  
With this arrangement we will be able to see the metrics of all the data metrics in a single graph.

But,with the above arrangement we will be seeing separate graphs for each of the nodes separately. If we want  the graphs to be tied to a single node, inside the proemtheus.yml file, we can set up the scarpe_config and static_config sections we can put in all the ip addresses in a single target node.

With prometheus option above, we dont have the option to store all the graphs, metrics and other data . To do this job we need to make use of Grafana. Grafana is a centralized monitoring tool where we can see the data metircs of all the tools in a single place.

Data from prom is given to Grafana. It is a monitoring dash board tool.

We download grafana by starting a new instance.

We need to download it and unzip it using the tar command
wget https://dl.grafana.com/enterprise/release/grafana-enterprise-10.1.1.linux-amd64.tar.gz
tar -zxvf grafana-enterprise-10.1.1.linux-amd64.tar.gz
   
Please note that grafana runs on 3000 port . It has to be started by going in to the bin folder and with command ./grafana-server web.

We can then open the grafana webpage and in port 3000 in our server ans set up a basic password (as admin and admin).

From the grafana webpage, we can go to the Datasources section and then select prometheus. Or we can choose the dashboard and give prometheus as the

data source.

Then we can enter the queries for all the nodes in the network and check the graphs in the grafana dashboard.

Grafana also provides default template to get all the queries . We can check them in grafana template variable webpage We can select the required

variable's .json file (provided with ID), download it in our machine and then in the grafana UI we can import it in to the dashboard.

Generally the metrics available for the node exporter variable are :

CPU usage, Memory usage, Network Received,Network transmitted,Disk I/O, Disk Space usage etc.

We can also setuop an alerting mechanism by setting up a threshold value  for a particular metric and give our email ID in alert notifications.

Biut we also need to do e-mail setting in both gmail settings and also in sample.ni in conf folder.


 
 
  
 
  
  

   







  



 














 













































